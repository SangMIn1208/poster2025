[
  {
    "objectID": "posts/2.시각화및VAR적합.html",
    "href": "posts/2.시각화및VAR적합.html",
    "title": "2. 시각화 및 VAR 적합",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\nwt= pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nwt.columns = ['date', 'tem', 'rain', 'wind', 'hum', 'sol']\n\n- 전체 기간 시도표\n\nfor i in range(5):\n    wt.iloc[:,[i+1]].plot(figsize=(15, 3));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 첫 1주일 시도표\n\nfor i in range(5):\n    wt.iloc[:24*7,[i+1]].plot(figsize=(15, 3));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n#ts = pd.Series(dff.iloc[:24*7,[0+1]].values, index=pd.date_range(start='2024-01-01', periods=24, freq='H'))\n\n# 분해 수행 (Additive 모델, 주기성 주기=24시간)\nresult = seasonal_decompose(wt.iloc[:24*14,[0+1]].values, model='additive', period=24)\n\n# 시각화\nresult.plot()\n#plt.suptitle(\"시계열 분해 결과 (추세 + 계절 + 잡음)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nts=wt.iloc[:24*14,[0+1]].values\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\nplot_acf(ts, lags=50, ax=axes[0])\naxes[0].set_title(\"ACF\")\n\nplot_pacf(ts, lags=50, ax=axes[1])\naxes[1].set_title(\"PACF\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nwt.iloc[:,1:].corr()\n\n\n\n\n\n\n\n\ntem\nrain\nwind\nhum\nsol\n\n\n\n\ntem\n1.000000\n0.051619\n0.016475\n-0.056472\n0.348940\n\n\nrain\n0.051619\n1.000000\n0.045720\n0.184801\n-0.070628\n\n\nwind\n0.016475\n0.045720\n1.000000\n-0.311978\n0.284515\n\n\nhum\n-0.056472\n0.184801\n-0.311978\n1.000000\n-0.620501\n\n\nsol\n0.348940\n-0.070628\n0.284515\n-0.620501\n1.000000\n\n\n\n\n\n\n\n\nVAR 모형에 적합\n- \\(X\\) : y의 과거값 및 외생변수, 길이는 24*14\n- \\(y\\) : 예측하고자 하는 일사량, 길이는 24\n\n온도와 일사량\n\n\nX = wt.iloc[:24*14,[1,-1]]\ny = wt.iloc[24*14:24*15,1:]\n\n\nfrom statsmodels.tsa.api import VAR\nfrom sklearn.metrics import mean_squared_error\n# 데이터: y, x1~x4가 있는 시계열 데이터프레임\n# 인덱스는 datetime이어야 함\n\n# 1. 정상성 검사 및 필요시 차분 (여기선 생략, 필요시 추가해줘)\n\n# 2. VAR 모형 적합\nmodel = VAR(X)\n\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=48)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\n#print(results.summary())\n\nAIC 기준 최적 시차: 21\n\n\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast1 = results.forecast(y=forecast_input, steps=n_forecast)\n\n\n습도와 일사량\n\n\nX = wt.iloc[:24*14,[-2,-1]]\n#y = wt.iloc[24*7:24*8,1:]\n\nmodel = VAR(X)\n-0\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=24)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\nresults.summary()\n\nAIC 기준 최적 시차: 21\n\n\n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Tue, 01, Apr, 2025\nTime:                     07:26:53\n--------------------------------------------------------------------\nNo. of Equations:         2.00000    BIC:                  0.0867873\nNobs:                     315.000    HQIC:                 -0.528393\nLog likelihood:          -660.240    FPE:                   0.392863\nAIC:                    -0.937725    Det(Omega_mle):        0.304156\n--------------------------------------------------------------------\nResults for equation hum\n==========================================================================\n             coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------\nconst          12.877262         3.048607            4.224           0.000\nL1.hum          1.034009         0.064842           15.947           0.000\nL1.sol         -6.288329         1.841606           -3.415           0.001\nL2.hum         -0.085306         0.091430           -0.933           0.351\nL2.sol          2.476996         2.761977            0.897           0.370\nL3.hum         -0.105987         0.091506           -1.158           0.247\nL3.sol          4.269964         2.810271            1.519           0.129\nL4.hum          0.104119         0.091715            1.135           0.256\nL4.sol         -3.066122         2.814244           -1.090           0.276\nL5.hum          0.020022         0.091322            0.219           0.826\nL5.sol          0.726784         2.790890            0.260           0.795\nL6.hum         -0.154890         0.090401           -1.713           0.087\nL6.sol         -1.001202         2.651443           -0.378           0.706\nL7.hum          0.063325         0.090179            0.702           0.483\nL7.sol         -1.789725         2.574931           -0.695           0.487\nL8.hum          0.013035         0.090204            0.145           0.885\nL8.sol          2.753985         2.559143            1.076           0.282\nL9.hum         -0.079469         0.090351           -0.880           0.379\nL9.sol         -2.534441         2.565794           -0.988           0.323\nL10.hum        -0.021562         0.090809           -0.237           0.812\nL10.sol         0.092242         2.566511            0.036           0.971\nL11.hum         0.101221         0.090510            1.118           0.263\nL11.sol         0.270904         2.559397            0.106           0.916\nL12.hum        -0.044597         0.090874           -0.491           0.624\nL12.sol        -0.931457         2.592624           -0.359           0.719\nL13.hum        -0.049559         0.090879           -0.545           0.586\nL13.sol         0.019081         2.638249            0.007           0.994\nL14.hum         0.071637         0.090692            0.790           0.430\nL14.sol        -1.801443         2.654979           -0.679           0.497\nL15.hum        -0.159492         0.091383           -1.745           0.081\nL15.sol        -1.397162         2.654903           -0.526           0.599\nL16.hum         0.130378         0.091520            1.425           0.154\nL16.sol         3.921220         2.642805            1.484           0.138\nL17.hum         0.125505         0.091874            1.366           0.172\nL17.sol        -0.658568         2.657127           -0.248           0.804\nL18.hum        -0.039232         0.092146           -0.426           0.670\nL18.sol         0.307973         2.647252            0.116           0.907\nL19.hum        -0.033336         0.091821           -0.363           0.717\nL19.sol        -2.210109         2.647698           -0.835           0.404\nL20.hum        -0.091010         0.091946           -0.990           0.322\nL20.sol         1.522105         2.596331            0.586           0.558\nL21.hum         0.060905         0.065948            0.924           0.356\nL21.sol        -3.586148         1.718873           -2.086           0.037\n==========================================================================\n\nResults for equation sol\n==========================================================================\n             coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------\nconst          -0.013084         0.108455           -0.121           0.904\nL1.hum         -0.004107         0.002307           -1.780           0.075\nL1.sol          1.190143         0.065516           18.166           0.000\nL2.hum          0.001161         0.003253            0.357           0.721\nL2.sol         -0.319998         0.098258           -3.257           0.001\nL3.hum          0.003093         0.003255            0.950           0.342\nL3.sol         -0.013066         0.099976           -0.131           0.896\nL4.hum         -0.002233         0.003263           -0.685           0.494\nL4.sol         -0.105982         0.100118           -1.059           0.290\nL5.hum          0.000845         0.003249            0.260           0.795\nL5.sol          0.007535         0.099287            0.076           0.940\nL6.hum          0.004978         0.003216            1.548           0.122\nL6.sol         -0.002949         0.094326           -0.031           0.975\nL7.hum         -0.002640         0.003208           -0.823           0.410\nL7.sol          0.000148         0.091604            0.002           0.999\nL8.hum         -0.003149         0.003209           -0.981           0.326\nL8.sol          0.058175         0.091042            0.639           0.523\nL9.hum          0.002759         0.003214            0.858           0.391\nL9.sol          0.043017         0.091279            0.471           0.637\nL10.hum        -0.000294         0.003231           -0.091           0.927\nL10.sol        -0.032885         0.091305           -0.360           0.719\nL11.hum        -0.000207         0.003220           -0.064           0.949\nL11.sol        -0.080112         0.091052           -0.880           0.379\nL12.hum         0.001996         0.003233            0.617           0.537\nL12.sol        -0.010646         0.092234           -0.115           0.908\nL13.hum         0.002650         0.003233            0.820           0.412\nL13.sol         0.054054         0.093857            0.576           0.565\nL14.hum        -0.007051         0.003226           -2.185           0.029\nL14.sol         0.042851         0.094452            0.454           0.650\nL15.hum         0.004732         0.003251            1.456           0.146\nL15.sol         0.005935         0.094449            0.063           0.950\nL16.hum        -0.003122         0.003256           -0.959           0.338\nL16.sol        -0.081783         0.094019           -0.870           0.384\nL17.hum         0.000985         0.003268            0.301           0.763\nL17.sol         0.018094         0.094528            0.191           0.848\nL18.hum        -0.000796         0.003278           -0.243           0.808\nL18.sol        -0.070061         0.094177           -0.744           0.457\nL19.hum         0.003162         0.003267            0.968           0.333\nL19.sol        -0.020653         0.094193           -0.219           0.826\nL20.hum        -0.002413         0.003271           -0.738           0.461\nL20.sol         0.080876         0.092365            0.876           0.381\nL21.hum         0.000457         0.002346            0.195           0.846\nL21.sol         0.107119         0.061150            1.752           0.080\n==========================================================================\n\nCorrelation matrix of residuals\n            hum       sol\nhum    1.000000 -0.387709\nsol   -0.387709  1.000000\n\n\n\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast2 = results.forecast(y=forecast_input, steps=n_forecast)\n\n\n풍속과 일사량\n\n\nX = wt.iloc[:24*14,[-3,-1]]\n#y = wt.iloc[24*7:24*8,1:]\n\nmodel = VAR(X)\n\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=24)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\nresults.summary()\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast2_ = results.forecast(y=forecast_input, steps=n_forecast)\n\nAIC 기준 최적 시차: 21\n\n\n\n강수량과 일사량\n\n\nX = wt.iloc[:24*14,[-4,-1]]\n#y = wt.iloc[24*7:24*8,1:]\n\nmodel = VAR(X)\n\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=24)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\nresults.summary()\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast3_ = results.forecast(y=forecast_input, steps=n_forecast)\n\nAIC 기준 최적 시차: 20\n\n\n\nAR 모형 일사량만 이용\n\n\nfrom statsmodels.tsa.ar_model import AutoReg\n\n# 예시: y 시계열 데이터프레임 (datetime index 권장)\n# df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=True)\n# y = df['y']\ny_ = wt.iloc[:24*14,[-1]]# 시계열 형태로 가져오기\n\n# 1. 시차(p) 설정 또는 자동 선택\np = 24  # 최근 24시간을 기반으로 다음을 예측한다고 가정\n\n# 2. AR 모형 적합\nmodel = AutoReg(y_, lags=p, old_names=False)\nresults = model.fit()\n\n# 3. 다음 24시간 예측\nforecast3 = results.predict(start=len(y), end=len(y)+23)\n\n\nprint('온도, 일사량',mean_squared_error(y['sol'].values,forecast1[:,1]))\nprint('습도, 일사량',mean_squared_error(y['sol'].values,forecast2[:,1]))\nprint('풍속, 일사량',mean_squared_error(y['sol'].values,forecast2_[:,1]))\nprint('강수량, 일사량',mean_squared_error(y['sol'].values,forecast3_[:,1]))\n\nprint('일사량',mean_squared_error(y['sol'].values,forecast3))\n\n\n\nplt.figure(figsize = (15,7))\nplt.plot(y['sol'].values, label = 'y',color = 'black')\nplt.plot(forecast1[:,1],label = 'tem+sol')\nplt.plot(forecast2[:,1],label = 'hum+sol')\nplt.plot(forecast2_[:,1],label = 'wind+sol')\nplt.plot(forecast3_[:,1],label = 'rain+sol')\n\nplt.plot(forecast3.values,label = 'sol')\nplt.legend()\nplt.show()\n\n온도, 일사량 0.20502706789905156\n습도, 일사량 0.2600785050783011\n풍속, 일사량 0.08792570082191763\n강수량, 일사량 0.3652957905544259\n일사량 0.4390186400449338\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 0. y_ 인덱스가 숫자면 → datetime 인덱스로 바꿔주기\nif not isinstance(y_.index[0], pd.Timestamp):\n    y_.index = pd.date_range(start='2024-01-01', periods=len(y_), freq='h')\n\n# 1. 예측 구간 인덱스 생성\nforecast_index = pd.date_range(start=y_.index[-1] + pd.Timedelta(hours=1), periods=24, freq='h')\n\n# 2. 예측값 시리즈화\nsol = pd.Series(y['sol'].values, index=forecast_index)\ntem_sol = pd.Series(forecast1[:, 1], index=forecast_index)\nhum_sol = pd.Series(forecast2[:, 1], index=forecast_index)\nsol_only = pd.Series(forecast3.values.flatten(), index=forecast_index)\n\n# 3. 플롯\nplt.figure(figsize=(15, 7))\nplt.plot(y_[-24*7:],color = 'black')\nplt.plot(sol, label = 'y',color = 'black')\n#plt.plot(tem_sol, label='tem+sol')\nplt.plot(hum_sol, label='hum+sol')\nplt.plot(sol_only, label='sol only')\nplt.legend()\n#plt.title(\"과거 + 예측 시계열\")\n#plt.xlabel(\"시간\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n- Granger 인과검정\n\n각 변수 상관관계\n풀모형에서 Granger 인과검정\n\n\\(H_0\\) : 외생변수 전체는 Granger 인과하지 않다\n\\(H_1\\) : 외생변수 중 어떤 하나 이상의 변수가 Granger 인과하다\n\n\n\nfrom statsmodels.tsa.api import VAR\n\n# 1. 전체 시계열 데이터 사용 (예: df = ['y', 'x1', 'x2', 'x3', 'x4'])\n\nX = wt.iloc[:24*14,1:]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic')  # 자동 시차 선택\n\ngc_test = results.test_causality(caused='sol', causing=['tem', 'rain', 'wind', 'hum'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: ['tem', 'rain', 'wind', 'hum'] do not Granger-cause sol. Conclusion: reject H_0 at 5% significance level.\n================================================\nTest statistic Critical value p-value     df    \n------------------------------------------------\n         2.683          1.758   0.001 (12, 1585)\n------------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[1,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['tem'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: tem does not Granger-cause sol. Conclusion: reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n         2.323          1.590   0.001 (20, 550)\n-----------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[2,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['rain'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: rain does not Granger-cause sol. Conclusion: reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n         3.284          1.590   0.000 (20, 550)\n-----------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[3,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['wind'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: wind does not Granger-cause sol. Conclusion: fail to reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n        0.8684          1.575   0.633 (21, 544)\n-----------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[4,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['hum'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: hum does not Granger-cause sol. Conclusion: fail to reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n        0.9374          1.575   0.542 (21, 544)\n-----------------------------------------------\n\n\n\nfig, ax = plt.subplots(5,1,figsize=(12,10))\nfor i in range(5):\n    ax[i].plot(wt.iloc[:24*14,i+1])"
  },
  {
    "objectID": "posts/4.NeuralGrangerCausality.html",
    "href": "posts/4.NeuralGrangerCausality.html",
    "title": "4. Neural Granger Causality",
    "section": "",
    "text": "링크 : Neural Granger Causality\n\n2. Linear Granger causality\n- Granger causality : 한 시계열의 과거 값이 다른 시계열이 미래 값을 예측하는데 도움이 되는지를 평가하는 개념\n- VAR(Vector Autogressive Regression)모델\n\nGranger causality를 분석하는 가장 기본적인 방법\n시계열 데이터 표현 \\(x_t=\\sum_{k=1}^{K}A^{(k)}x_{t-k}+\\epsilon_t\\)\n\\(x_{t}\\) : 시점 t에서의 다변량 시계열 데이터(p개의 변수 포함)\nk : 최대 시차\n\\(A^{(k)}\\) : 시차 k에서의 계수행렬(pxp크기)\n\\(e_t\\) : 평균이 0인 잡음\n\n- 인과성 판단 기준 - 특정 시계열 j가 다른 시계열 i의 미래를 예측하는데 기여하는지를 확인\n\n\\(A^k_{ij}=0\\)이면, 시계열 j는 시계열 i에 대해 Granger 비인과적\n\\(A^k_{ij}\\neq 0\\)이면, 시계열 j는 시계열 i에 대해 Granger 인과적\n\n- 희소성 유도 패널티 적용\n\n너무 많은 변수를 고려하면 과적합문제 발생-&gt;Lasso,Group Lasso 패널티를 적용하여 불필요한 계수 0으로\n\nGroup Lasso 패널티 : 결과적으로 희소한 Granger 인과 네트워크를 학습\n\n\\(min_{A(1),...,A(K)}\\sum_{t=K}^{T}||x_t-\\sum_{k=1}^{K}A^{(k)}x_{t-k}||^2_2+\\lambda\\sum_{ij}||(A^{(1)}_{ij},...,A^{K}_{ij}||_2,\\)\n\n계층적 패널티 : 너무 긴 시차를 사용하면 과적합이 발생할 수 있으므로 모델이 적절한 시차로 학습해아함\n\n\n- VAR은 선형적 관계에서만 가정, 비선형 데이터는 한계\n\n\\(\\to\\) Neural Granger Causality 기법 필요\n\n\n\n3. Models for Neural Granger Causality\n\n3.1 Adapting Neural Networks for Granger Causality\n- 비선형 자기 회귀 모델(Nonlinear Autogressive Model, NAR)\n\n\\(x_t=g(x_{&lt;t1},...,x_{&lt;tp})+\\epsilon_t\\)\n\\(x_t\\) : 시점 t에서의 다변량 시계열 데이터\n\\(x_{&lt;t1},...,x_{&lt;tp}\\) : 과거 p개의 시점에서 수집된 데이터\n\\(g(*)\\) : 비선형 함수(ex: 신경망)\n\\(e_t : 평균이 0인 잡음(noise)\\)\n\n- Problem\n\n블랙박스 문제 : MLP/LSTM은 강력한 예측성능을 가지지만 학습된 모델이 어떤 변수의 영향을 받는지 해석하기 어려움\n시간지연(Lag) 선택문제 : 전통적인 VAR모델에서는 최대 시차 k를 설정해야 하는데, 신경망 모델에서는 이를 자동으로 결정하는 방법이 필요\n\n- 해결방법\n\ncMLP,cLSTM : Component-wise Neural Network\n아이디어 : 각 출력 변수 \\(x_{ti}\\)마다 독립적인 신경망 \\(g_i\\)를 사용하여 입력변수와의 관계를 학습\n기존과 다르게 입력 가중치에 대한 희소성 패널티를 추가해 Granger Causality 분석가능\n예시 : \\(x_{ti}=g_i(x_{&lt;t1},...,x_{&lt;tp})+e_t\\)\n\\(g_i(*)\\) 변수 \\(i\\)에 대한 신경망 모델, 특정변수 \\(j\\)가 \\(i\\)에 영향을 미치지 않는다면, \\(g_{-i}\\)에서 \\(x_{&lt;tj}\\)에 대해 불변해야함\n\n- 3가지 모델 소개\n\ncLSTM\ncMLP\ncRNN"
  },
  {
    "objectID": "posts/1.2024기상데이터전처리blog.html",
    "href": "posts/1.2024기상데이터전처리blog.html",
    "title": "1. 2024 기상데이터 전처리",
    "section": "",
    "text": "import pandas as pd\n\n- 데이터 불러오기\n\nwt= pd.read_csv(\"OBS_ASOS_TIM_20250322224121.csv\",encoding=\"cp949\")\n\n\nwt\n\n\n\n\n\n\n\n\n지점\n지점명\n일시\n기온(°C)\n강수량(mm)\n풍속(m/s)\n습도(%)\n일사(MJ/m2)\n\n\n\n\n0\n146\n전주\n2024-01-01 01:00\n3.8\nNaN\n1.5\n80\nNaN\n\n\n1\n146\n전주\n2024-01-01 02:00\n3.9\nNaN\n0.2\n79\nNaN\n\n\n2\n146\n전주\n2024-01-01 03:00\n3.5\nNaN\n0.4\n84\nNaN\n\n\n3\n146\n전주\n2024-01-01 04:00\n1.9\nNaN\n1.1\n92\nNaN\n\n\n4\n146\n전주\n2024-01-01 05:00\n1.4\nNaN\n1.5\n94\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n146\n전주\n2024-12-30 20:00\n7.6\nNaN\n1.4\n71\nNaN\n\n\n8756\n146\n전주\n2024-12-30 21:00\n7.5\nNaN\n1.7\n69\nNaN\n\n\n8757\n146\n전주\n2024-12-30 22:00\n7.2\nNaN\n1.2\n70\nNaN\n\n\n8758\n146\n전주\n2024-12-30 23:00\n7.2\nNaN\n1.7\n71\nNaN\n\n\n8759\n146\n전주\n2024-12-31 00:00\n7.4\nNaN\n2.8\n70\nNaN\n\n\n\n\n8760 rows × 8 columns\n\n\n\n- null값 확인\n\nwt.isnull().sum()\n\n지점              0\n지점명             0\n일시              0\n기온(°C)          0\n강수량(mm)      7822\n풍속(m/s)         0\n습도(%)           0\n일사(MJ/m2)    3967\ndtype: int64\n\n\n\nwt.columns\n\nIndex(['지점', '지점명', '일시', '기온(°C)', '강수량(mm)', '풍속(m/s)', '습도(%)',\n       '일사(MJ/m2)'],\n      dtype='object')\n\n\n- 필요없는 변수 제거\n\nwt=wt.drop(columns=['지점명','지점'])\n\n\nwt.columns\n\nIndex(['일시', '기온(°C)', '강수량(mm)', '풍속(m/s)', '습도(%)', '일사(MJ/m2)'], dtype='object')\n\n\n- column명 쉽게 변경\n\nwt.columns=['일시','기온','강수량','풍속','습도','일사']\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\nNaN\n1.5\n80\nNaN\n\n\n1\n2024-01-01 02:00\n3.9\nNaN\n0.2\n79\nNaN\n\n\n2\n2024-01-01 03:00\n3.5\nNaN\n0.4\n84\nNaN\n\n\n3\n2024-01-01 04:00\n1.9\nNaN\n1.1\n92\nNaN\n\n\n4\n2024-01-01 05:00\n1.4\nNaN\n1.5\n94\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\nNaN\n1.4\n71\nNaN\n\n\n8756\n2024-12-30 21:00\n7.5\nNaN\n1.7\n69\nNaN\n\n\n8757\n2024-12-30 22:00\n7.2\nNaN\n1.2\n70\nNaN\n\n\n8758\n2024-12-30 23:00\n7.2\nNaN\n1.7\n71\nNaN\n\n\n8759\n2024-12-31 00:00\n7.4\nNaN\n2.8\n70\nNaN\n\n\n\n\n8760 rows × 6 columns\n\n\n\n- null값 제거\n- 강수량, 일사 변수만 null값이 있기때문에 비가 안올 때, 밤에 0으로 측정이 안되었다고 판단\n- 0으로 채움\n\nwt[['강수량', '일사']] = wt[['강수량', '일사']].fillna(0)\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 그래프 크기 설정\nfig, axes = plt.subplots(3, 2, figsize=(12, 12))\naxes = axes.flatten()\n\n# 각 변수별 시각화\nfor i, col in enumerate(wt.columns[1:]):\n    sns.lineplot(data=wt[col], marker='o', ax=axes[i])\n    axes[i].set_title(col)\n    axes[i].set_ylabel(col)\n\n# 빈 그래프 삭제\nfig.delaxes(axes[-1])  \n\nplt.tight_layout()\nplt.show()\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n- csv 생성\n\nwt.to_csv('weather2024.csv', index=False, encoding='utf-8-sig')"
  },
  {
    "objectID": "posts/3.GrangerCausality.html",
    "href": "posts/3.GrangerCausality.html",
    "title": "3. Granger Causality",
    "section": "",
    "text": "링크 : Granger Causality"
  },
  {
    "objectID": "posts/3.GrangerCausality.html#granger-causality-a-review-and-recend-advances",
    "href": "posts/3.GrangerCausality.html#granger-causality-a-review-and-recend-advances",
    "title": "3. Granger Causality",
    "section": "Granger Causality : A Review and Recend Advances",
    "text": "Granger Causality : A Review and Recend Advances\n- 다양한 분야에서 시계열 간의 상호작용을 이해하는 것이 중요\n- 우리는 시계열 간의 인과적 상호작용을 이해하고자 하지만 실험이 불가능하고 현상에 대한 기계적 모델도 없는 상황에서 관측 데이터에 기반한 추론만 가능하다는 제한이 존재\n\n(미래를 모르기 때문에)\n\n- 이러한 경우, 시계열에 내재된 시간 순서성을 활용해, 과거가 미래에 영향을 준다는 방향으로 제한된 인과적 설명을 이끌어내는 프레임워크 제안\n- 시계열 \\(y_{t}\\) 의 과거 값이 다른 시계열 \\(x_{t}\\) 의 미래값을 얼마나 잘 예측할 수 있는지에 기반하여 인과성의 개념을 제안\n\n제한된 모델 : $Y_{t}=+ _{i=1}^{p} iY{t-i} + _t $\n\n과거의 Y 값만 사용하여 \\(Y_t\\)를 예측하는 모델\n\n확장된 모델 : $Y_{t}=+ {i=1}^{p} iY{t-i} + {j=1}^{q} Y_jX_{t-j} + _t $\n\n과거의 Y값 뿐만 아니라 X의 과거 값까지 사용하여 예측\n\n\n- 제약\n\nreal-valued time series with\nlinear dynamics dependent on\na knwn number of past lagged observations, with\nobservations available at a fixed, discrete sampling rate that matches the time scale of the causal structure of interest\n\n- “Granger Causality”는 단순한 예측 가능성을 의미\n\nGranger 에서 말하는 인과성은 진정한 인과관계 X\nY의 과거 정보를 포함해서 X의 예측오차가 줄어들면 Y가 X의 Granger causality\n\n\\(H&lt;t\\) 시간 t 이전의 모든 정보 집합\n\\(P(X_t|H&lt;t)\\) 해당 정보 집합을 통한 x의 최적의 예측\n\\(var(x_t-P(x_t|H&lt;t))\\) $ &lt; $ \\(var(x_t-P(x_t|H&lt;t / y&lt;t)\\)\n\n\n- VAR 모델 기반 Granger 모델\n\n\\(A^0x_t=\\sum_{k=1}^{d}A^kX_{t-k}+\\epsilon_{t}\\)\n\n시간 t에서의 변수 벡터를 \\(X_{t} =(x_1t, x_2t, ...x_pt)^T\\)\n\\(A_{k}\\)는 시차(lag) : k에 대한 회귀계수 행렬\n\\(\\epsilon_t\\) : 백색 잡음\n\n\n- Granger 인과성의 기본가정(제한점) &lt;- 분석을 하기위해선 아래가정 충족해야함\n\n연속형 시계열\n선형성\n이산된 시간\n고정된 시차\n정상성\n완전한 관측(오차가 없어야함)\n관련된 모든 변수가 포함되어야함\n\n- 초기 Granger 인과성 검증 방법\n\n이변량 모델\n\n\\(a_{0x}*x_t=\\sum_{k=1}^{d}a^{(k)}_{xx}x_{t-k}+\\sum_{k=1}^{d}a^{(k)}_{xy}y_{t-k}+e_{t,x}\\)\n\\(a_{0y}*y_t=\\sum_{k=1}^{d}a^{(k)}_{yy}y_{t-k}+\\sum_{k=1}^{d}a^{(k)}_{yx}x_{t-k}+e_{t,y}\\)\n\nReduced Model, Full model을 비교해 F검정 실시\n\n\\(F=\\frac{(RSS_{red}-RSS_{full}/(r-s))}{RSS_{full}/(T-r)}\\)\n귀무가설 \\(H_{0} : Y_{t}는 X_{t} granger 원인이 아니다(Y_j=0)\\)\n대립가설 \\(H_{1} : Y_{t}는 X_{t} granger 원인이다(Y_j\\neq0)\\)\n\n\n- 전통적으로 선형 VAR 모델 가정으로 기반\n\n하지만 현실의 시스템은 훨씬 복잡하고 전통적 프레임은 한계점 존재\n\n이변량 모델을 사용하여 고차원 데이터는 정확하 분석 어려움\n비선형성과 비정규성 문제(VAR모델은 이러한 특성 반영 불가)\n불규칙 샘플링 및 관측 데이터 문제(고정된 시간 간격으로 수집, 현실에서는 다양한 주기로 변화 발생)\n\n\n- 해결을 위한 현대의 시도\n\n네트워크 Granger 인과성 : 여러개의 시계열 변수를 동시에 고려하여 다변량 인과성 분석\n비선형, 고차원 데이터 처리 : Lasso 및 Group Lasso 를 활용한 고차원 VAR모델, 딥러닝, 머신러닝 기법으로 비선형 인과성 분석\n비정상 시계열 및 혼합 주파수 데이터 처리 :\n\n현실에서는 시계열 데이터가 동일하게 수집되지 않는 문제 해결기법 개발\n다중 시간 척도에서 데이터를 통합하여 Granger 인과성 분석\n\n\n- 네트워크 기반 Granger Causality\n\n다변량 시계열에서 여러 변수 간의 상호작용을 고려하여 인과관계 분석\n\n이변량이 아닌 전체 변수(외생변수들의 영향 고려) 네트워크를 분석\nVAR 모델 활용 * \\(x_t=\\sum_{k=1}^{d}A^kX_{t-k}+\\epsilon_t\\) * \\(A_k\\)의 특정요소가 0이 아니면 해당 변수간의 인과성이 존재한다고 판단\n\n\n- 고차원 VAR 모델에서 변수 선택\n\n외생변수 포함하는 경우\n\nFAVOR\nm차원의 요인 \\(f_t\\)를 포함\n직접 관측 X, 최대가능도법 MLE, PCA로 추정\n\n내생변수가 매우 많은 경우\n\n축소 추정으로 VAR 학습\n베이지안 방법론 -&gt; 큰 계수 줄임\nLasso 와 같은 희소성 기법 활용\n\n특정 계수 \\(A_k\\)를 0으로 강제하여 중요하지 않은 변수 제거\n튜닝 파라미터 \\(\\lambda \\geq 0\\) 이 희소성 수준 조절\n값이 클수록 많은 계수 0으로\n$A^k_{ji} $인 k가 있어야함\n\nVAR 모델 손실함수를 정규화하여 해결하는 방식\n\n\n- 이후의 시도들\n\nBasu et al.\nDavis et al"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "poster2025",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 2, 2025\n\n\n7. cRNN\n\n\n이상민 \n\n\n\n\nApr 2, 2025\n\n\n5. cLSTM\n\n\n이상민 \n\n\n\n\nApr 2, 2025\n\n\n6. cMLP\n\n\n이상민 \n\n\n\n\nApr 1, 2025\n\n\n4. Neural Granger Causality\n\n\n이상민 \n\n\n\n\nMar 30, 2025\n\n\n3. Granger Causality\n\n\n이상민 \n\n\n\n\nMar 29, 2025\n\n\n2. 시각화 및 VAR 적합\n\n\n이상민 \n\n\n\n\nMar 24, 2025\n\n\n1. 2024 기상데이터 전처리\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/7.cRNN.html",
    "href": "posts/7.cRNN.html",
    "title": "7. cRNN",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom synthetic import simulate_lorenz_96\nfrom models.crnn import cRNN, train_model_ista\n\n\nimport pandas as pd\n\n\n# For GPU acceleration\ndevice = torch.device('cuda')\n\n\n# Simulate data\n#X_np, GC = simulate_lorenz_96(p=10, F=10, T=1000)\n#X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt = pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nX_np = wt.iloc[:, 1:].values\n\n\nX = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\n# Set up model\ncrnn = cRNN(X.shape[-1], hidden=100).cuda(device=device)\n\n\n# Train with ISTA\ntrain_loss_list = train_model_ista(\n    crnn, X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n    check_every=50)\n\n----------Iter = 50----------\nLoss = nan\nVariable usage = 8.00%\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[28], line 2\n      1 # Train with ISTA\n----&gt; 2 train_loss_list = train_model_ista(\n      3     crnn, X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n      4     check_every=50)\n\nFile ~/python_project/3-1/poster/Neural-GC/models/crnn.py:505, in train_model_ista(crnn, X, context, lr, max_iter, lam, lam_ridge, lookback, check_every, verbose)\n    503     best_it = it\n    504     best_model = deepcopy(crnn)\n--&gt; 505 elif (it - best_it) == lookback * check_every:\n    506     if verbose:\n    507         print('Stopping early')\n\nTypeError: unsupported operand type(s) for -: 'int' and 'NoneType'"
  },
  {
    "objectID": "posts/5.cLSTM.html",
    "href": "posts/5.cLSTM.html",
    "title": "5. cLSTM",
    "section": "",
    "text": "1. cMLP, cRNN, cLSTM\n- cMLP\n\n완전연결 신견망(MLP) 기반\n과거의 관측값을 입력으로 받아 비선형 함수를 통해 출력 생성\n시계열 데이터에서, 시간적 정보는 고려하지 않고 독립적인 입력변수로 다룸\n장점 : 계산량이 적고 단순\n단점 : 시계열 특성 고려x(순서정보 반영하지 못함) \\(\\to\\) 장기 의존성 학습이 어려움\n\\(\\hat{X}_{t+1}=f_{\\theta}(X_t,X_{t-1},...,X_{t-k})\\)\n\n\\(f_{\\theta}\\) : MLP, k : 과거 시점 개수\n\n\n- cRNN\n\n순환 신경망(RNN)을 기반, 시간 의존성 반영할 수 있음\n과거 데이터의 상태를 은닉 상태에 저장하여 학습\n장점 : 시계열 표현 잘 반영\n단점 : 장기 의존성을 학습하기는 어려움(Vanishing Gradient 문제)\n\\(h_t=\\sigma(W_th_{t-1}+W_xX_t)\\)\n\\(\\hat{X}_{t+1}=f_{\\theta}(h_t)\\)\n\\(h_t\\) : 은닉 상태, \\(W_t, W_x\\) 는 가중치 행렬, \\(\\sigma\\) 는 활성화 함수\n\n- cLSTM\n\n장기-단기 기억 네트워크를 기반으로 한 모델\nRNN 단점인 Vanishing Gradient 문제를 해결하여 장기 의존성 학습 가능\n셀 상태와 게이트 구조를 사용하여 중요한 정보만 저장하고 불필요한 정보 삭제\n단점 : 연산량이 많아 학습속도가 느릴 수 있음\n\\(f_t = \\sigma(W_f[h_{t-1},X_t]+b_f\\)\n\\(i_t = \\sigma(W_i[h_{t-1},X_t]+b_i\\)\n\\(o_t = \\sigma(W_o[h_{t-1},X_t]+b_o\\)\n\\(C_t=f_t \\odot C_{t-1}+i_t \\odot \\tilde{C}_t\\)\n\\(h_t=o_t\\odot tanh(C_t)\\)\n\\(f_t\\) : forget gate, \\(i_t\\) : input gate, \\(o_t\\) : output gate, \\(C_t\\) : 셀 상태\n\n\n\nNeural Granger Causality Github\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom synthetic import simulate_lorenz_96\nfrom models.clstm import cLSTM, train_model_ista\n\n\nimport pandas as pd\n\n\n# For GPU acceleration\ndevice = torch.device('cuda')\n\n- simulate_lorenz_96 함수는 Lorenz-96 모델을 시뮬레이션하여 데이터를 생성하는 함수\n\n기상학과 동역학 시스템에서 사용되는 비선형 동적 시스템 모델\n아래의 미분 방정식으로 정의\n\n\\(\\frac{dx_i}{dt}=(x_{i+1}-x_{i-2})x_{i-1}-x_i+F\\)\n\n비선형 시스템으로 복잡한 시간적 변화를 모사\n기상 모델링과 같은 분야에서 사용\n혼돈 현상을 나타낼 수 있음\np : 시스템의 차원(변수 개수), T: 시뮬레이션 시간길이, F : 외부 힘(혼돈 유발 파라미터)\n\n\n# Simulate data\n#X_np, GC = simulate_lorenz_96(p=10, F=10, T=1000)\n#X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt = pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n- numpy 형태의 x,y지정\n\nX_np = wt.iloc[:, 1:].values\n\n- tensor형태로 변환\n\nX = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt.columns\n\nIndex(['일시', '기온', '강수량', '풍속', '습도', '일사'], dtype='object')\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n\n# 전체 시계열 데이터 (기온, 강수량, 풍속, 습도)\naxarr[0].plot(X_np)\naxarr[0].set_xlabel('T')\naxarr[0].set_title('Entire time series')\naxarr[0].legend(wt.columns[1:])  # 범례 추가\n\n# 처음 50개 타임스텝 데이터\naxarr[1].plot(X_np[:50])\naxarr[1].set_xlabel('T')\naxarr[1].set_title('First 50 time points')\naxarr[1].legend(wt.columns[1:])  # 범례 추가\n\n# 레이아웃 정리 및 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n- X.shape[-1] : 컬럼의 개수, 여기서는 5\n- hidden : LSTM의 은닉 상태 크기\n\n# Set up model\nclstm = cLSTM(X.shape[-1], hidden=100).cuda(device=device)\n\n- context=n : 과거 n개의 타임스텝을 고려하여 학습\n- lam, lam_ridge : 정규화 관련 하이퍼파라미터..그대로 둬도 무방\n- lr : learning rate\n- max_iter : 최대 몇 번의 반복동안 학습할지\n- check_every : 모델 학습 과정에서 일정 간격마다 검증을 수행하는 주기\n\n# Train with ISTA\ntrain_loss_list = train_model_ista(\n    clstm, X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n    check_every=50)\n\n----------Iter = 50----------\nLoss = 113.947586\nVariable usage = 100.00%\n----------Iter = 100----------\nLoss = 95.600487\nVariable usage = 100.00%\n----------Iter = 150----------\nLoss = 85.747925\nVariable usage = 28.00%\n----------Iter = 200----------\nLoss = 78.164307\nVariable usage = 24.00%\n----------Iter = 250----------\nLoss = 80.287163\nVariable usage = 32.00%\n----------Iter = 300----------\nLoss = 76.854881\nVariable usage = 32.00%\n----------Iter = 350----------\nLoss = 70.780518\nVariable usage = 28.00%\n----------Iter = 400----------\nLoss = 62.113892\nVariable usage = 20.00%\n----------Iter = 450----------\nLoss = 67.783348\nVariable usage = 20.00%\n----------Iter = 500----------\nLoss = 68.171379\nVariable usage = 28.00%\n----------Iter = 550----------\nLoss = 65.656067\nVariable usage = 20.00%\n----------Iter = 600----------\nLoss = 57.279652\nVariable usage = 24.00%\n----------Iter = 650----------\nLoss = 62.956081\nVariable usage = 20.00%\n----------Iter = 700----------\nLoss = 60.115559\nVariable usage = 24.00%\n----------Iter = 750----------\nLoss = 62.895275\nVariable usage = 20.00%\n----------Iter = 800----------\nLoss = 83.166695\nVariable usage = 24.00%\n----------Iter = 850----------\nLoss = 77.377487\nVariable usage = 20.00%\nStopping early\n\n\n- 1000번 이하에서 early stopping이 발생\n\nloss값이 충분이 줄어들었거나\nloss 감소가 멈추었거나\n\n- 이부분에서 데이터가 cuda 의 텐서여서 오류가 발생해 cpu, numpy로 변환\n\ntrain_loss_np = np.array([loss.cpu().item() for loss in train_loss_list])\n\n\n# Loss function plot\nplt.figure(figsize=(8, 5))\nplt.plot(50 * np.arange(len(train_loss_np)), train_loss_np)\nplt.title('cLSTM training')\nplt.ylabel('Loss')\nplt.xlabel('Training steps')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 5개의 기상 변수 (기온, 강수량, 풍속, 습도, 일사)에 대한 GC 행렬\nGC = np.array([[1, 1, 0, 0, 0],  # 기온 -&gt; 강수량\n               [0, 1, 1, 0, 0],  # 강수량 -&gt; 풍속\n               [0, 0, 1, 1, 0],  # 풍속 -&gt; 습도\n               [0, 0, 0, 1, 1],  # 습도 -&gt; 일사\n               [0, 0, 0, 0, 1]])  # 일사 -&gt; 기온 (self loop)\n\n\n# Check learned Granger causality\nGC_est = clstm.GC().cpu().data.numpy()\n\nprint('True variable usage = %.2f%%' % (100 * np.mean(GC)))\nprint('Estimated variable usage = %.2f%%' % (100 * np.mean(GC_est)))\nprint('Accuracy = %.2f%%' % (100 * np.mean(GC == GC_est)))\n\n# Make figures\nfig, axarr = plt.subplots(1, 2, figsize=(10, 5))\naxarr[0].imshow(GC, cmap='Blues')\naxarr[0].set_title('GC actual')\naxarr[0].set_ylabel('Affected series')\naxarr[0].set_xlabel('Causal series')\naxarr[0].set_xticks([])\naxarr[0].set_yticks([])\n\naxarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\naxarr[1].set_ylabel('Affected series')\naxarr[1].set_xlabel('Causal series')\naxarr[1].set_xticks([])\naxarr[1].set_yticks([])\n\n# Mark disagreements\nfor i in range(len(GC_est)):\n    for j in range(len(GC_est)):\n        if GC[i, j] != GC_est[i, j]:\n            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n            axarr[1].add_patch(rect)\n\nplt.show()\n\nTrue variable usage = 36.00%\nEstimated variable usage = 24.00%\nAccuracy = 64.00%\n\n\n\n\n\n\n\n\n\n- Granger Causality\n\n순서 : (기온 강수량 풍속 습도 일사)\nGC 실제 변수 간의 인과 관계, GC_est 학습된 모델이 추정한 인과 관계\n실제 GC와 추정된 GC_est를 비교하여 정확도(accuracy) 계산\n불일치하는 인과관계는 빨간색으로 표시\n실제 GC를 어떻게 설정해야하지…..\n왜이렇게 많이 틀렸지?? 뭔가 잘못 됐나"
  },
  {
    "objectID": "posts/6.cMLP.html",
    "href": "posts/6.cMLP.html",
    "title": "6. cMLP",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom synthetic import simulate_var\nfrom models.cmlp import cMLP, cMLPSparse, train_model_ista, train_unregularized\n\n\nimport pandas as pd\n\n\n# For GPU acceleration\ndevice = torch.device('cuda')\n\n\n# Simulate data\n#X_np, beta, GC = simulate_var(p=10, T=1000, lag=3)\n#X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt = pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nX_np = wt.iloc[:, 1:].values\n\n\nX = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\n# Set up model\ncmlp = cMLP(X.shape[-1], lag=5, hidden=[100]).cuda(device=device)\n\n\n# Train with ISTA\ntrain_loss_list = train_model_ista(\n    cmlp, X, lam=0.002, lam_ridge=1e-2, lr=5e-2, penalty='H', max_iter=50000,\n    check_every=100)\n\n----------Iter = 100----------\nLoss = nan\nVariable usage = 80.00%\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 2\n      1 # Train with ISTA\n----&gt; 2 train_loss_list = train_model_ista(\n      3     cmlp, X, lam=0.002, lam_ridge=1e-2, lr=5e-2, penalty='H', max_iter=50000,\n      4     check_every=100)\n\nFile ~/python_project/3-1/poster/Neural-GC/models/cmlp.py:507, in train_model_ista(cmlp, X, lr, max_iter, lam, lam_ridge, penalty, lookback, check_every, verbose)\n    505     best_it = it\n    506     best_model = deepcopy(cmlp)\n--&gt; 507 elif (it - best_it) == lookback * check_every:\n    508     if verbose:\n    509         print('Stopping early')\n\nTypeError: unsupported operand type(s) for -: 'int' and 'NoneType'"
  }
]