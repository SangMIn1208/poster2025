[
  {
    "objectID": "posts/8.Lorenz_time_table.html",
    "href": "posts/8.Lorenz_time_table.html",
    "title": "5. cLSTM",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom synthetic import simulate_lorenz_96\nfrom models.clstm import cLSTM, train_model_ista\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# simulate_lorenz_96 함수는 이미 구현되어 있다고 가정합니다.\n\n# ✅ 실험 조건\np = 20\nF_list = [10, 40]\nT_list = [250, 500, 1000]\n\n# ✅ 그림 만들기: 2 (F 값) x 3 (T 값) subplot 구성\nfig, axes = plt.subplots(len(F_list), len(T_list), figsize=(15, 6), sharex=False, sharey=True)\nfig.suptitle(\"Lorenz-96 (변수 0 시도표) - 다양한 F, T\", fontsize=16)\n\nfor i, F in enumerate(F_list):\n    for j, T in enumerate(T_list):\n        # 데이터 시뮬레이션: 반환값에서 실제 데이터만 선택\n        X, _ = simulate_lorenz_96(p=p, F=F, T=T)\n        \n        # 변수 0만 시도표로 시각화: np.tile을 이용해 10행으로 반복\n        ax = axes[i, j]\n        im = ax.imshow(np.tile(X[:, 0], (10, 1)), aspect='auto', cmap='viridis', origin='lower')\n        ax.set_title(f\"F={F}, T={T}\")\n        ax.set_yticks([])\n        ax.set_xlabel(\"Time\")\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.88)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# simulate_lorenz_96 함수는 이미 있다고 가정\n\np = 20\nF_list = [10, 40]\nT_list = [250, 500, 1000]\n\n# 그림 만들기\nfig, axes = plt.subplots(len(F_list), len(T_list), figsize=(15, 6), sharex=False, sharey=True)\nfig.suptitle(\"Lorenz-96 (변수 0 선 그래프) - 다양한 F, T\", fontsize=16)\n\nfor i, F in enumerate(F_list):\n    for j, T in enumerate(T_list):\n        # 시뮬레이션\n        X, _ = simulate_lorenz_96(p=p, F=F, T=T)\n\n        # 선 그래프로 시각화\n        ax = axes[i, j]\n        ax.plot(X[:, 0], color='tab:blue')\n        ax.set_title(f\"F={F}, T={T}\")\n        ax.set_xlabel(\"Time\")\n        if j == 0:\n            ax.set_ylabel(\"x₀ value\")\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.88)\nplt.show()\n\n\n/tmp/ipykernel_196839/3375532367.py:27: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  plt.tight_layout()\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)"
  },
  {
    "objectID": "posts/6.cMLP.html",
    "href": "posts/6.cMLP.html",
    "title": "6. cMLP",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom synthetic import simulate_var\nfrom models.cmlp import cMLP, cMLPSparse, train_model_ista, train_unregularized\n\n\nimport pandas as pd\n\n\n# For GPU acceleration\ndevice = torch.device('cuda')\n\n\n# Simulate data\n#X_np, beta, GC = simulate_var(p=10, T=1000, lag=3)\n#X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt = pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nX_np = wt.iloc[:, 1:].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_np)\n\nX_np = X_scaled.astype(np.float32)\nX = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nX = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\n# Set up model\ncmlp = cMLP(X.shape[-1], lag=5, hidden=[100]).cuda(device=device)\n\n\n# Train with ISTA\ntrain_loss_list = train_model_ista(\n    cmlp, X, lam=0.002, lam_ridge=1e-2, lr=5e-2, penalty='H', max_iter=50000,\n    check_every=100)\n\n----------Iter = 100----------\nLoss = 0.379958\nVariable usage = 100.00%\n----------Iter = 200----------\nLoss = 0.366504\nVariable usage = 100.00%\n----------Iter = 300----------\nLoss = 0.357959\nVariable usage = 100.00%\n----------Iter = 400----------\nLoss = 0.351531\nVariable usage = 100.00%\n----------Iter = 500----------\nLoss = 0.346289\nVariable usage = 100.00%\n----------Iter = 600----------\nLoss = 0.341924\nVariable usage = 100.00%\n----------Iter = 700----------\nLoss = 0.338129\nVariable usage = 100.00%\n----------Iter = 800----------\nLoss = 0.334756\nVariable usage = 100.00%\n----------Iter = 900----------\nLoss = 0.331626\nVariable usage = 100.00%\n----------Iter = 1000----------\nLoss = 0.328709\nVariable usage = 100.00%\n----------Iter = 1100----------\nLoss = 0.325968\nVariable usage = 100.00%\n----------Iter = 1200----------\nLoss = 0.323389\nVariable usage = 100.00%\n----------Iter = 1300----------\nLoss = 0.320968\nVariable usage = 100.00%\n----------Iter = 1400----------\nLoss = 0.318670\nVariable usage = 100.00%\n----------Iter = 1500----------\nLoss = 0.316470\nVariable usage = 100.00%\n----------Iter = 1600----------\nLoss = 0.314376\nVariable usage = 100.00%\n----------Iter = 1700----------\nLoss = 0.312334\nVariable usage = 100.00%\n----------Iter = 1800----------\nLoss = 0.310358\nVariable usage = 100.00%\n----------Iter = 1900----------\nLoss = 0.308462\nVariable usage = 100.00%\n----------Iter = 2000----------\nLoss = 0.306619\nVariable usage = 100.00%\n----------Iter = 2100----------\nLoss = 0.304820\nVariable usage = 100.00%\n----------Iter = 2200----------\nLoss = 0.303053\nVariable usage = 100.00%\n----------Iter = 2300----------\nLoss = 0.301335\nVariable usage = 100.00%\n----------Iter = 2400----------\nLoss = 0.299677\nVariable usage = 100.00%\n----------Iter = 2500----------\nLoss = 0.298070\nVariable usage = 100.00%\n----------Iter = 2600----------\nLoss = 0.296501\nVariable usage = 100.00%\n----------Iter = 2700----------\nLoss = 0.294969\nVariable usage = 100.00%\n----------Iter = 2800----------\nLoss = 0.293476\nVariable usage = 100.00%\n----------Iter = 2900----------\nLoss = 0.292024\nVariable usage = 100.00%\n----------Iter = 3000----------\nLoss = 0.290616\nVariable usage = 100.00%\n----------Iter = 3100----------\nLoss = 0.289244\nVariable usage = 100.00%\n----------Iter = 3200----------\nLoss = 0.287913\nVariable usage = 100.00%\n----------Iter = 3300----------\nLoss = 0.286624\nVariable usage = 100.00%\n----------Iter = 3400----------\nLoss = 0.285378\nVariable usage = 100.00%\n----------Iter = 3500----------\nLoss = 0.284173\nVariable usage = 100.00%\n----------Iter = 3600----------\nLoss = 0.282993\nVariable usage = 100.00%\n----------Iter = 3700----------\nLoss = 0.281854\nVariable usage = 100.00%\n----------Iter = 3800----------\nLoss = 0.280750\nVariable usage = 100.00%\n----------Iter = 3900----------\nLoss = 0.279678\nVariable usage = 100.00%\n----------Iter = 4000----------\nLoss = 0.278645\nVariable usage = 100.00%\n----------Iter = 4100----------\nLoss = 0.277644\nVariable usage = 100.00%\n----------Iter = 4200----------\nLoss = 0.276675\nVariable usage = 100.00%\n----------Iter = 4300----------\nLoss = 0.275716\nVariable usage = 100.00%\n----------Iter = 4400----------\nLoss = 0.274802\nVariable usage = 100.00%\n----------Iter = 4500----------\nLoss = 0.273929\nVariable usage = 100.00%\n----------Iter = 4600----------\nLoss = 0.273084\nVariable usage = 100.00%\n----------Iter = 4700----------\nLoss = 0.272277\nVariable usage = 100.00%\n----------Iter = 4800----------\nLoss = 0.271502\nVariable usage = 100.00%\n----------Iter = 4900----------\nLoss = 0.270757\nVariable usage = 100.00%\n----------Iter = 5000----------\nLoss = 0.270052\nVariable usage = 100.00%\n----------Iter = 5100----------\nLoss = 0.269368\nVariable usage = 100.00%\n----------Iter = 5200----------\nLoss = 0.268712\nVariable usage = 100.00%\n----------Iter = 5300----------\nLoss = 0.268086\nVariable usage = 100.00%\n----------Iter = 5400----------\nLoss = 0.267494\nVariable usage = 100.00%\n----------Iter = 5500----------\nLoss = 0.266921\nVariable usage = 100.00%\n----------Iter = 5600----------\nLoss = 0.266371\nVariable usage = 100.00%\n----------Iter = 5700----------\nLoss = 0.265842\nVariable usage = 100.00%\n----------Iter = 5800----------\nLoss = 0.265335\nVariable usage = 100.00%\n----------Iter = 5900----------\nLoss = 0.264853\nVariable usage = 100.00%\n----------Iter = 6000----------\nLoss = 0.264386\nVariable usage = 100.00%\n----------Iter = 6100----------\nLoss = 0.263937\nVariable usage = 100.00%\n----------Iter = 6200----------\nLoss = 0.263507\nVariable usage = 100.00%\n----------Iter = 6300----------\nLoss = 0.263096\nVariable usage = 100.00%\n----------Iter = 6400----------\nLoss = 0.262691\nVariable usage = 100.00%\n----------Iter = 6500----------\nLoss = 0.262301\nVariable usage = 100.00%\n----------Iter = 6600----------\nLoss = 0.261925\nVariable usage = 100.00%\n----------Iter = 6700----------\nLoss = 0.261558\nVariable usage = 100.00%\n----------Iter = 6800----------\nLoss = 0.261206\nVariable usage = 100.00%\n----------Iter = 6900----------\nLoss = 0.260862\nVariable usage = 100.00%\n----------Iter = 7000----------\nLoss = 0.260534\nVariable usage = 100.00%\n----------Iter = 7100----------\nLoss = 0.260217\nVariable usage = 100.00%\n----------Iter = 7200----------\nLoss = 0.259909\nVariable usage = 100.00%\n----------Iter = 7300----------\nLoss = 0.259618\nVariable usage = 100.00%\n----------Iter = 7400----------\nLoss = 0.259329\nVariable usage = 100.00%\n----------Iter = 7500----------\nLoss = 0.259053\nVariable usage = 100.00%\n----------Iter = 7600----------\nLoss = 0.258782\nVariable usage = 100.00%\n----------Iter = 7700----------\nLoss = 0.258515\nVariable usage = 100.00%\n----------Iter = 7800----------\nLoss = 0.258254\nVariable usage = 100.00%\n----------Iter = 7900----------\nLoss = 0.258009\nVariable usage = 100.00%\n----------Iter = 8000----------\nLoss = 0.257770\nVariable usage = 100.00%\n----------Iter = 8100----------\nLoss = 0.257540\nVariable usage = 100.00%\n----------Iter = 8200----------\nLoss = 0.257317\nVariable usage = 100.00%\n----------Iter = 8300----------\nLoss = 0.257095\nVariable usage = 100.00%\n----------Iter = 8400----------\nLoss = 0.256880\nVariable usage = 100.00%\n----------Iter = 8500----------\nLoss = 0.256674\nVariable usage = 100.00%\n----------Iter = 8600----------\nLoss = 0.256462\nVariable usage = 100.00%\n----------Iter = 8700----------\nLoss = 0.256259\nVariable usage = 100.00%\n----------Iter = 8800----------\nLoss = 0.256053\nVariable usage = 100.00%\n----------Iter = 8900----------\nLoss = 0.255860\nVariable usage = 100.00%\n----------Iter = 9000----------\nLoss = 0.255664\nVariable usage = 100.00%\n----------Iter = 9100----------\nLoss = 0.255475\nVariable usage = 100.00%\n----------Iter = 9200----------\nLoss = 0.255306\nVariable usage = 100.00%\n----------Iter = 9300----------\nLoss = 0.255131\nVariable usage = 100.00%\n----------Iter = 9400----------\nLoss = 0.254968\nVariable usage = 100.00%\n----------Iter = 9500----------\nLoss = 0.254805\nVariable usage = 100.00%\n----------Iter = 9600----------\nLoss = 0.254650\nVariable usage = 100.00%\n----------Iter = 9700----------\nLoss = 0.254500\nVariable usage = 100.00%\n----------Iter = 9800----------\nLoss = 0.254353\nVariable usage = 100.00%\n----------Iter = 9900----------\nLoss = 0.254206\nVariable usage = 100.00%\n----------Iter = 10000----------\nLoss = 0.254064\nVariable usage = 100.00%\n----------Iter = 10100----------\nLoss = 0.253922\nVariable usage = 100.00%\n----------Iter = 10200----------\nLoss = 0.253786\nVariable usage = 100.00%\n----------Iter = 10300----------\nLoss = 0.253656\nVariable usage = 100.00%\n----------Iter = 10400----------\nLoss = 0.253515\nVariable usage = 100.00%\n----------Iter = 10500----------\nLoss = 0.253387\nVariable usage = 100.00%\n----------Iter = 10600----------\nLoss = 0.253266\nVariable usage = 100.00%\n----------Iter = 10700----------\nLoss = 0.253135\nVariable usage = 100.00%\n----------Iter = 10800----------\nLoss = 0.253016\nVariable usage = 100.00%\n----------Iter = 10900----------\nLoss = 0.252899\nVariable usage = 100.00%\n----------Iter = 11000----------\nLoss = 0.252779\nVariable usage = 100.00%\n----------Iter = 11100----------\nLoss = 0.252669\nVariable usage = 100.00%\n----------Iter = 11200----------\nLoss = 0.252551\nVariable usage = 100.00%\n----------Iter = 11300----------\nLoss = 0.252438\nVariable usage = 100.00%\n----------Iter = 11400----------\nLoss = 0.252332\nVariable usage = 100.00%\n----------Iter = 11500----------\nLoss = 0.252228\nVariable usage = 100.00%\n----------Iter = 11600----------\nLoss = 0.252122\nVariable usage = 100.00%\n----------Iter = 11700----------\nLoss = 0.252016\nVariable usage = 100.00%\n----------Iter = 11800----------\nLoss = 0.251921\nVariable usage = 100.00%\n----------Iter = 11900----------\nLoss = 0.251814\nVariable usage = 100.00%\n----------Iter = 12000----------\nLoss = 0.251719\nVariable usage = 100.00%\n----------Iter = 12100----------\nLoss = 0.251626\nVariable usage = 100.00%\n----------Iter = 12200----------\nLoss = 0.251529\nVariable usage = 100.00%\n----------Iter = 12300----------\nLoss = 0.251438\nVariable usage = 100.00%\n----------Iter = 12400----------\nLoss = 0.251341\nVariable usage = 100.00%\n----------Iter = 12500----------\nLoss = 0.251262\nVariable usage = 100.00%\n----------Iter = 12600----------\nLoss = 0.251164\nVariable usage = 100.00%\n----------Iter = 12700----------\nLoss = 0.251074\nVariable usage = 100.00%\n----------Iter = 12800----------\nLoss = 0.250992\nVariable usage = 100.00%\n----------Iter = 12900----------\nLoss = 0.250908\nVariable usage = 100.00%\n----------Iter = 13000----------\nLoss = 0.250820\nVariable usage = 100.00%\n----------Iter = 13100----------\nLoss = 0.250733\nVariable usage = 100.00%\n----------Iter = 13200----------\nLoss = 0.250655\nVariable usage = 100.00%\n----------Iter = 13300----------\nLoss = 0.250579\nVariable usage = 100.00%\n----------Iter = 13400----------\nLoss = 0.250491\nVariable usage = 100.00%\n----------Iter = 13500----------\nLoss = 0.250420\nVariable usage = 100.00%\n----------Iter = 13600----------\nLoss = 0.250328\nVariable usage = 100.00%\n----------Iter = 13700----------\nLoss = 0.250268\nVariable usage = 100.00%\n----------Iter = 13800----------\nLoss = 0.250190\nVariable usage = 100.00%\n----------Iter = 13900----------\nLoss = 0.250108\nVariable usage = 100.00%\n----------Iter = 14000----------\nLoss = 0.250039\nVariable usage = 100.00%\n----------Iter = 14100----------\nLoss = 0.249963\nVariable usage = 100.00%\n----------Iter = 14200----------\nLoss = 0.249891\nVariable usage = 100.00%\n----------Iter = 14300----------\nLoss = 0.249828\nVariable usage = 100.00%\n----------Iter = 14400----------\nLoss = 0.249764\nVariable usage = 100.00%\n----------Iter = 14500----------\nLoss = 0.249700\nVariable usage = 100.00%\n----------Iter = 14600----------\nLoss = 0.249631\nVariable usage = 100.00%\n----------Iter = 14700----------\nLoss = 0.249570\nVariable usage = 100.00%\n----------Iter = 14800----------\nLoss = 0.249516\nVariable usage = 100.00%\n----------Iter = 14900----------\nLoss = 0.249455\nVariable usage = 100.00%\n----------Iter = 15000----------\nLoss = 0.249397\nVariable usage = 100.00%\n----------Iter = 15100----------\nLoss = 0.249346\nVariable usage = 100.00%\n----------Iter = 15200----------\nLoss = 0.249282\nVariable usage = 100.00%\n----------Iter = 15300----------\nLoss = 0.249226\nVariable usage = 100.00%\n----------Iter = 15400----------\nLoss = 0.249174\nVariable usage = 100.00%\n----------Iter = 15500----------\nLoss = 0.249128\nVariable usage = 100.00%\n----------Iter = 15600----------\nLoss = 0.249078\nVariable usage = 100.00%\n----------Iter = 15700----------\nLoss = 0.249025\nVariable usage = 100.00%\n----------Iter = 15800----------\nLoss = 0.248975\nVariable usage = 100.00%\n----------Iter = 15900----------\nLoss = 0.248933\nVariable usage = 100.00%\n----------Iter = 16000----------\nLoss = 0.248872\nVariable usage = 100.00%\n----------Iter = 16100----------\nLoss = 0.248828\nVariable usage = 100.00%\n----------Iter = 16200----------\nLoss = 0.248800\nVariable usage = 100.00%\n----------Iter = 16300----------\nLoss = 0.248757\nVariable usage = 100.00%\n----------Iter = 16400----------\nLoss = 0.248709\nVariable usage = 100.00%\n----------Iter = 16500----------\nLoss = 0.248643\nVariable usage = 100.00%\n----------Iter = 16600----------\nLoss = 0.248617\nVariable usage = 100.00%\n----------Iter = 16700----------\nLoss = 0.248577\nVariable usage = 100.00%\n----------Iter = 16800----------\nLoss = 0.248544\nVariable usage = 100.00%\n----------Iter = 16900----------\nLoss = 0.248504\nVariable usage = 100.00%\n----------Iter = 17000----------\nLoss = 0.248460\nVariable usage = 100.00%\n----------Iter = 17100----------\nLoss = 0.248419\nVariable usage = 100.00%\n----------Iter = 17200----------\nLoss = 0.248357\nVariable usage = 100.00%\n----------Iter = 17300----------\nLoss = 0.248330\nVariable usage = 100.00%\n----------Iter = 17400----------\nLoss = 0.248306\nVariable usage = 100.00%\n----------Iter = 17500----------\nLoss = 0.248273\nVariable usage = 100.00%\n----------Iter = 17600----------\nLoss = 0.248247\nVariable usage = 100.00%\n----------Iter = 17700----------\nLoss = 0.248212\nVariable usage = 100.00%\n----------Iter = 17800----------\nLoss = 0.248180\nVariable usage = 100.00%\n----------Iter = 17900----------\nLoss = 0.248152\nVariable usage = 100.00%\n----------Iter = 18000----------\nLoss = 0.248091\nVariable usage = 100.00%\n----------Iter = 18100----------\nLoss = 0.248080\nVariable usage = 100.00%\n----------Iter = 18200----------\nLoss = 0.248074\nVariable usage = 100.00%\n----------Iter = 18300----------\nLoss = 0.248029\nVariable usage = 100.00%\n----------Iter = 18400----------\nLoss = 0.247971\nVariable usage = 100.00%\n----------Iter = 18500----------\nLoss = 0.247962\nVariable usage = 100.00%\n----------Iter = 18600----------\nLoss = 0.247935\nVariable usage = 100.00%\n----------Iter = 18700----------\nLoss = 0.247866\nVariable usage = 100.00%\n----------Iter = 18800----------\nLoss = 0.247889\nVariable usage = 100.00%\n----------Iter = 18900----------\nLoss = 0.247866\nVariable usage = 100.00%\n----------Iter = 19000----------\nLoss = 0.247844\nVariable usage = 100.00%\n----------Iter = 19100----------\nLoss = 0.247832\nVariable usage = 100.00%\n----------Iter = 19200----------\nLoss = 0.247849\nVariable usage = 100.00%\n----------Iter = 19300----------\nLoss = 0.247865\nVariable usage = 100.00%\n----------Iter = 19400----------\nLoss = 0.247840\nVariable usage = 100.00%\n----------Iter = 19500----------\nLoss = 0.247837\nVariable usage = 100.00%\n----------Iter = 19600----------\nLoss = 0.247862\nVariable usage = 100.00%\nStopping early\n\n\n\ntrain_loss_np = np.array([loss.cpu().item() for loss in train_loss_list])\n\n\n# Loss function plot\nplt.figure(figsize=(8, 5))\nplt.plot(50 * np.arange(len(train_loss_np)), train_loss_np)\nplt.title('cLSTM training')\nplt.ylabel('Loss')\nplt.xlabel('Training steps')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 5개의 기상 변수 (기온, 강수량, 풍속, 습도, 일사)에 대한 GC 행렬\nGC = np.array([[1, 1, 0, 0, 0],  # 기온 -&gt; 강수량\n               [0, 1, 1, 0, 0],  # 강수량 -&gt; 풍속\n               [0, 0, 1, 1, 0],  # 풍속 -&gt; 습도\n               [0, 0, 0, 1, 1],  # 습도 -&gt; 일사\n               [0, 0, 0, 0, 1]])  # 일사 -&gt; 기온 (self loop)\n\n\n# Check learned Granger causality\nGC_est = cmlp.GC().cpu().data.numpy()\n\nprint('True variable usage = %.2f%%' % (100 * np.mean(GC)))\nprint('Estimated variable usage = %.2f%%' % (100 * np.mean(GC_est)))\nprint('Accuracy = %.2f%%' % (100 * np.mean(GC == GC_est)))\n\n# Make figures\nfig, axarr = plt.subplots(1, 2, figsize=(10, 5))\naxarr[0].imshow(GC, cmap='Blues')\naxarr[0].set_title('GC actual')\naxarr[0].set_ylabel('Affected series')\naxarr[0].set_xlabel('Causal series')\naxarr[0].set_xticks([])\naxarr[0].set_yticks([])\n\naxarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\naxarr[1].set_ylabel('Affected series')\naxarr[1].set_xlabel('Causal series')\naxarr[1].set_xticks([])\naxarr[1].set_yticks([])\n\n# Mark disagreements\nfor i in range(len(GC_est)):\n    for j in range(len(GC_est)):\n        if GC[i, j] != GC_est[i, j]:\n            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n            axarr[1].add_patch(rect)\n\nplt.show()\n\nTrue variable usage = 36.00%\nEstimated variable usage = 100.00%\nAccuracy = 36.00%"
  },
  {
    "objectID": "posts/5.cLSTM.html",
    "href": "posts/5.cLSTM.html",
    "title": "5. cLSTM",
    "section": "",
    "text": "1. cMLP, cRNN, cLSTM\n- cMLP\n\n완전연결 신견망(MLP) 기반\n과거의 관측값을 입력으로 받아 비선형 함수를 통해 출력 생성\n시계열 데이터에서, 시간적 정보는 고려하지 않고 독립적인 입력변수로 다룸\n장점 : 계산량이 적고 단순\n단점 : 시계열 특성 고려x(순서정보 반영하지 못함) \\(\\to\\) 장기 의존성 학습이 어려움\n\\(\\hat{X}_{t+1}=f_{\\theta}(X_t,X_{t-1},...,X_{t-k})\\)\n\n\\(f_{\\theta}\\) : MLP, k : 과거 시점 개수\n\n\n- cRNN\n\n순환 신경망(RNN)을 기반, 시간 의존성 반영할 수 있음\n과거 데이터의 상태를 은닉 상태에 저장하여 학습\n장점 : 시계열 표현 잘 반영\n단점 : 장기 의존성을 학습하기는 어려움(Vanishing Gradient 문제)\n\\(h_t=\\sigma(W_th_{t-1}+W_xX_t)\\)\n\\(\\hat{X}_{t+1}=f_{\\theta}(h_t)\\)\n\\(h_t\\) : 은닉 상태, \\(W_t, W_x\\) 는 가중치 행렬, \\(\\sigma\\) 는 활성화 함수\n\n- cLSTM\n\n장기-단기 기억 네트워크를 기반으로 한 모델\nRNN 단점인 Vanishing Gradient 문제를 해결하여 장기 의존성 학습 가능\n셀 상태와 게이트 구조를 사용하여 중요한 정보만 저장하고 불필요한 정보 삭제\n단점 : 연산량이 많아 학습속도가 느릴 수 있음\n\\(f_t = \\sigma(W_f[h_{t-1},X_t]+b_f\\)\n\\(i_t = \\sigma(W_i[h_{t-1},X_t]+b_i\\)\n\\(o_t = \\sigma(W_o[h_{t-1},X_t]+b_o\\)\n\\(C_t=f_t \\odot C_{t-1}+i_t \\odot \\tilde{C}_t\\)\n\\(h_t=o_t\\odot tanh(C_t)\\)\n\\(f_t\\) : forget gate, \\(i_t\\) : input gate, \\(o_t\\) : output gate, \\(C_t\\) : 셀 상태\n\n\n\nNeural Granger Causality Github\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom synthetic import simulate_lorenz_96\nfrom models.clstm import cLSTM, train_model_ista\n\n\nimport pandas as pd\n\n\n# For GPU acceleration\ndevice = torch.device('cuda')\n\n- simulate_lorenz_96 함수는 Lorenz-96 모델을 시뮬레이션하여 데이터를 생성하는 함수\n\n기상학과 동역학 시스템에서 사용되는 비선형 동적 시스템 모델\n아래의 미분 방정식으로 정의\n\n\\(\\frac{dx_i}{dt}=(x_{i+1}-x_{i-2})x_{i-1}-x_i+F\\)\n\n비선형 시스템으로 복잡한 시간적 변화를 모사\n기상 모델링과 같은 분야에서 사용\n혼돈 현상을 나타낼 수 있음\np : 시스템의 차원(변수 개수), T: 시뮬레이션 시간길이, F : 외부 힘(혼돈 유발 파라미터)\n\n\n# Simulate data\n#X_np, GC = simulate_lorenz_96(p=10, F=10, T=1000)\n#X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt = pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n- 정규화 \\(\\to\\) numpy \\(\\to\\) Torch.tensor + 차원추가\n\nfrom sklearn.preprocessing import StandardScaler\n\nX_np = wt.iloc[:, 1:].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_np)\n\nX_np = X_scaled.astype(np.float32)\nX = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt.columns\n\nIndex(['일시', '기온', '강수량', '풍속', '습도', '일사'], dtype='object')\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize=(16, 5))\n\n# 전체 시계열 데이터 (기온, 강수량, 풍속, 습도)\naxarr[0].plot(X_np)\naxarr[0].set_xlabel('T')\naxarr[0].set_title('Entire time series')\naxarr[0].legend(wt.columns[1:])  # 범례 추가\n\n# 처음 50개 타임스텝 데이터\naxarr[1].plot(X_np[:50])\naxarr[1].set_xlabel('T')\naxarr[1].set_title('First 50 time points')\naxarr[1].legend(wt.columns[1:])  # 범례 추가\n\n# 레이아웃 정리 및 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n- X.shape[-1] : 컬럼의 개수, 여기서는 5\n- hidden : LSTM의 은닉 상태 크기\n\n# Set up model\nclstm = cLSTM(X.shape[-1], hidden=100).cuda(device=device)\n\n- context=n : 과거 n개의 타임스텝을 고려하여 학습\n- lam, lam_ridge : 정규화 관련 하이퍼파라미터..그대로 둬도 무방\n- lr : learning rate\n- max_iter : 최대 몇 번의 반복동안 학습할지\n- check_every : 모델 학습 과정에서 일정 간격마다 검증을 수행하는 주기\n\n# Train with ISTA\ntrain_loss_list = train_model_ista(\n    clstm, X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n    check_every=50)\n\n----------Iter = 50----------\nLoss = 35.149139\nVariable usage = 100.00%\n----------Iter = 100----------\nLoss = 10.155972\nVariable usage = 100.00%\n----------Iter = 150----------\nLoss = 2.331874\nVariable usage = 0.00%\n----------Iter = 200----------\nLoss = 2.328956\nVariable usage = 0.00%\n----------Iter = 250----------\nLoss = 2.326103\nVariable usage = 0.00%\n----------Iter = 300----------\nLoss = 2.323302\nVariable usage = 0.00%\n----------Iter = 350----------\nLoss = 2.320541\nVariable usage = 0.00%\n----------Iter = 400----------\nLoss = 2.317814\nVariable usage = 0.00%\n----------Iter = 450----------\nLoss = 2.315112\nVariable usage = 0.00%\n----------Iter = 500----------\nLoss = 2.312431\nVariable usage = 0.00%\n----------Iter = 550----------\nLoss = 2.309769\nVariable usage = 0.00%\n----------Iter = 600----------\nLoss = 2.307121\nVariable usage = 0.00%\n----------Iter = 650----------\nLoss = 2.304486\nVariable usage = 0.00%\n----------Iter = 700----------\nLoss = 2.301862\nVariable usage = 0.00%\n----------Iter = 750----------\nLoss = 2.299247\nVariable usage = 0.00%\n----------Iter = 800----------\nLoss = 2.296641\nVariable usage = 0.00%\n----------Iter = 850----------\nLoss = 2.294042\nVariable usage = 0.00%\n----------Iter = 900----------\nLoss = 2.291451\nVariable usage = 0.00%\n----------Iter = 950----------\nLoss = 2.288867\nVariable usage = 0.00%\n----------Iter = 1000----------\nLoss = 2.286288\nVariable usage = 0.00%\n----------Iter = 1050----------\nLoss = 2.283716\nVariable usage = 0.00%\n----------Iter = 1100----------\nLoss = 2.281150\nVariable usage = 0.00%\n----------Iter = 1150----------\nLoss = 2.278589\nVariable usage = 0.00%\n----------Iter = 1200----------\nLoss = 2.276034\nVariable usage = 0.00%\n----------Iter = 1250----------\nLoss = 2.273485\nVariable usage = 0.00%\n----------Iter = 1300----------\nLoss = 2.270941\nVariable usage = 0.00%\n----------Iter = 1350----------\nLoss = 2.268401\nVariable usage = 0.00%\n----------Iter = 1400----------\nLoss = 2.265868\nVariable usage = 0.00%\n----------Iter = 1450----------\nLoss = 2.263339\nVariable usage = 0.00%\n----------Iter = 1500----------\nLoss = 2.260815\nVariable usage = 0.00%\n----------Iter = 1550----------\nLoss = 2.258297\nVariable usage = 0.00%\n----------Iter = 1600----------\nLoss = 2.255784\nVariable usage = 0.00%\n----------Iter = 1650----------\nLoss = 2.253276\nVariable usage = 0.00%\n----------Iter = 1700----------\nLoss = 2.250772\nVariable usage = 0.00%\n----------Iter = 1750----------\nLoss = 2.248274\nVariable usage = 0.00%\n----------Iter = 1800----------\nLoss = 2.245781\nVariable usage = 0.00%\n----------Iter = 1850----------\nLoss = 2.243293\nVariable usage = 0.00%\n----------Iter = 1900----------\nLoss = 2.240810\nVariable usage = 0.00%\n----------Iter = 1950----------\nLoss = 2.238332\nVariable usage = 0.00%\n----------Iter = 2000----------\nLoss = 2.235858\nVariable usage = 0.00%\n----------Iter = 2050----------\nLoss = 2.233390\nVariable usage = 0.00%\n----------Iter = 2100----------\nLoss = 2.230927\nVariable usage = 0.00%\n----------Iter = 2150----------\nLoss = 2.228468\nVariable usage = 0.00%\n----------Iter = 2200----------\nLoss = 2.226015\nVariable usage = 0.00%\n----------Iter = 2250----------\nLoss = 2.223566\nVariable usage = 0.00%\n----------Iter = 2300----------\nLoss = 2.221122\nVariable usage = 0.00%\n----------Iter = 2350----------\nLoss = 2.218683\nVariable usage = 0.00%\n----------Iter = 2400----------\nLoss = 2.216249\nVariable usage = 0.00%\n----------Iter = 2450----------\nLoss = 2.213820\nVariable usage = 0.00%\n----------Iter = 2500----------\nLoss = 2.211396\nVariable usage = 0.00%\n----------Iter = 2550----------\nLoss = 2.208977\nVariable usage = 0.00%\n----------Iter = 2600----------\nLoss = 2.206562\nVariable usage = 0.00%\n----------Iter = 2650----------\nLoss = 2.204152\nVariable usage = 0.00%\n----------Iter = 2700----------\nLoss = 2.201747\nVariable usage = 0.00%\n----------Iter = 2750----------\nLoss = 2.199347\nVariable usage = 0.00%\n----------Iter = 2800----------\nLoss = 2.196952\nVariable usage = 0.00%\n----------Iter = 2850----------\nLoss = 2.194561\nVariable usage = 0.00%\n----------Iter = 2900----------\nLoss = 2.192176\nVariable usage = 0.00%\n----------Iter = 2950----------\nLoss = 2.189795\nVariable usage = 0.00%\n----------Iter = 3000----------\nLoss = 2.187418\nVariable usage = 0.00%\n----------Iter = 3050----------\nLoss = 2.185047\nVariable usage = 0.00%\n----------Iter = 3100----------\nLoss = 2.182680\nVariable usage = 0.00%\n----------Iter = 3150----------\nLoss = 2.180318\nVariable usage = 0.00%\n----------Iter = 3200----------\nLoss = 2.177961\nVariable usage = 0.00%\n----------Iter = 3250----------\nLoss = 2.175608\nVariable usage = 0.00%\n----------Iter = 3300----------\nLoss = 2.173260\nVariable usage = 0.00%\n----------Iter = 3350----------\nLoss = 2.170917\nVariable usage = 0.00%\n----------Iter = 3400----------\nLoss = 2.168579\nVariable usage = 0.00%\n----------Iter = 3450----------\nLoss = 2.166245\nVariable usage = 0.00%\n----------Iter = 3500----------\nLoss = 2.163915\nVariable usage = 0.00%\n----------Iter = 3550----------\nLoss = 2.161591\nVariable usage = 0.00%\n----------Iter = 3600----------\nLoss = 2.159271\nVariable usage = 0.00%\n----------Iter = 3650----------\nLoss = 2.156956\nVariable usage = 0.00%\n----------Iter = 3700----------\nLoss = 2.154645\nVariable usage = 0.00%\n----------Iter = 3750----------\nLoss = 2.152339\nVariable usage = 0.00%\n----------Iter = 3800----------\nLoss = 2.150038\nVariable usage = 0.00%\n----------Iter = 3850----------\nLoss = 2.147741\nVariable usage = 0.00%\n----------Iter = 3900----------\nLoss = 2.145449\nVariable usage = 0.00%\n----------Iter = 3950----------\nLoss = 2.143161\nVariable usage = 0.00%\n----------Iter = 4000----------\nLoss = 2.140878\nVariable usage = 0.00%\n----------Iter = 4050----------\nLoss = 2.138600\nVariable usage = 0.00%\n----------Iter = 4100----------\nLoss = 2.136326\nVariable usage = 0.00%\n----------Iter = 4150----------\nLoss = 2.134056\nVariable usage = 0.00%\n----------Iter = 4200----------\nLoss = 2.131791\nVariable usage = 0.00%\n----------Iter = 4250----------\nLoss = 2.129531\nVariable usage = 0.00%\n----------Iter = 4300----------\nLoss = 2.127275\nVariable usage = 0.00%\n----------Iter = 4350----------\nLoss = 2.125024\nVariable usage = 0.00%\n----------Iter = 4400----------\nLoss = 2.122777\nVariable usage = 0.00%\n----------Iter = 4450----------\nLoss = 2.120535\nVariable usage = 0.00%\n----------Iter = 4500----------\nLoss = 2.118297\nVariable usage = 0.00%\n----------Iter = 4550----------\nLoss = 2.116063\nVariable usage = 0.00%\n----------Iter = 4600----------\nLoss = 2.113835\nVariable usage = 0.00%\n----------Iter = 4650----------\nLoss = 2.111610\nVariable usage = 0.00%\n----------Iter = 4700----------\nLoss = 2.109390\nVariable usage = 0.00%\n----------Iter = 4750----------\nLoss = 2.107174\nVariable usage = 0.00%\n----------Iter = 4800----------\nLoss = 2.104963\nVariable usage = 0.00%\n----------Iter = 4850----------\nLoss = 2.102756\nVariable usage = 0.00%\n----------Iter = 4900----------\nLoss = 2.100554\nVariable usage = 0.00%\n----------Iter = 4950----------\nLoss = 2.098356\nVariable usage = 0.00%\n----------Iter = 5000----------\nLoss = 2.096163\nVariable usage = 0.00%\n----------Iter = 5050----------\nLoss = 2.093973\nVariable usage = 0.00%\n----------Iter = 5100----------\nLoss = 2.091789\nVariable usage = 0.00%\n----------Iter = 5150----------\nLoss = 2.089608\nVariable usage = 0.00%\n----------Iter = 5200----------\nLoss = 2.087432\nVariable usage = 0.00%\n----------Iter = 5250----------\nLoss = 2.085260\nVariable usage = 0.00%\n----------Iter = 5300----------\nLoss = 2.083093\nVariable usage = 0.00%\n----------Iter = 5350----------\nLoss = 2.080930\nVariable usage = 0.00%\n----------Iter = 5400----------\nLoss = 2.078771\nVariable usage = 0.00%\n----------Iter = 5450----------\nLoss = 2.076617\nVariable usage = 0.00%\n----------Iter = 5500----------\nLoss = 2.074467\nVariable usage = 0.00%\n----------Iter = 5550----------\nLoss = 2.072321\nVariable usage = 0.00%\n----------Iter = 5600----------\nLoss = 2.070179\nVariable usage = 0.00%\n----------Iter = 5650----------\nLoss = 2.068043\nVariable usage = 0.00%\n----------Iter = 5700----------\nLoss = 2.065909\nVariable usage = 0.00%\n----------Iter = 5750----------\nLoss = 2.063781\nVariable usage = 0.00%\n----------Iter = 5800----------\nLoss = 2.061656\nVariable usage = 0.00%\n----------Iter = 5850----------\nLoss = 2.059536\nVariable usage = 0.00%\n----------Iter = 5900----------\nLoss = 2.057420\nVariable usage = 0.00%\n----------Iter = 5950----------\nLoss = 2.055308\nVariable usage = 0.00%\n----------Iter = 6000----------\nLoss = 2.053200\nVariable usage = 0.00%\n----------Iter = 6050----------\nLoss = 2.051097\nVariable usage = 0.00%\n----------Iter = 6100----------\nLoss = 2.048998\nVariable usage = 0.00%\n----------Iter = 6150----------\nLoss = 2.046903\nVariable usage = 0.00%\n----------Iter = 6200----------\nLoss = 2.044812\nVariable usage = 0.00%\n----------Iter = 6250----------\nLoss = 2.042726\nVariable usage = 0.00%\n----------Iter = 6300----------\nLoss = 2.040644\nVariable usage = 0.00%\n----------Iter = 6350----------\nLoss = 2.038565\nVariable usage = 0.00%\n----------Iter = 6400----------\nLoss = 2.036491\nVariable usage = 0.00%\n----------Iter = 6450----------\nLoss = 2.034421\nVariable usage = 0.00%\n----------Iter = 6500----------\nLoss = 2.032356\nVariable usage = 0.00%\n----------Iter = 6550----------\nLoss = 2.030294\nVariable usage = 0.00%\n----------Iter = 6600----------\nLoss = 2.028236\nVariable usage = 0.00%\n----------Iter = 6650----------\nLoss = 2.026183\nVariable usage = 0.00%\n----------Iter = 6700----------\nLoss = 2.024134\nVariable usage = 0.00%\n----------Iter = 6750----------\nLoss = 2.022088\nVariable usage = 0.00%\n----------Iter = 6800----------\nLoss = 2.020047\nVariable usage = 0.00%\n----------Iter = 6850----------\nLoss = 2.018010\nVariable usage = 0.00%\n----------Iter = 6900----------\nLoss = 2.015977\nVariable usage = 0.00%\n----------Iter = 6950----------\nLoss = 2.013948\nVariable usage = 0.00%\n----------Iter = 7000----------\nLoss = 2.011923\nVariable usage = 0.00%\n----------Iter = 7050----------\nLoss = 2.009902\nVariable usage = 0.00%\n----------Iter = 7100----------\nLoss = 2.007886\nVariable usage = 0.00%\n----------Iter = 7150----------\nLoss = 2.005873\nVariable usage = 0.00%\n----------Iter = 7200----------\nLoss = 2.003864\nVariable usage = 0.00%\n----------Iter = 7250----------\nLoss = 2.001859\nVariable usage = 0.00%\n----------Iter = 7300----------\nLoss = 1.999858\nVariable usage = 0.00%\n----------Iter = 7350----------\nLoss = 1.997862\nVariable usage = 0.00%\n----------Iter = 7400----------\nLoss = 1.995869\nVariable usage = 0.00%\n----------Iter = 7450----------\nLoss = 1.993881\nVariable usage = 0.00%\n----------Iter = 7500----------\nLoss = 1.991896\nVariable usage = 0.00%\n----------Iter = 7550----------\nLoss = 1.989915\nVariable usage = 0.00%\n----------Iter = 7600----------\nLoss = 1.987938\nVariable usage = 0.00%\n----------Iter = 7650----------\nLoss = 1.985965\nVariable usage = 0.00%\n----------Iter = 7700----------\nLoss = 1.983996\nVariable usage = 0.00%\n----------Iter = 7750----------\nLoss = 1.982031\nVariable usage = 0.00%\n----------Iter = 7800----------\nLoss = 1.980070\nVariable usage = 0.00%\n----------Iter = 7850----------\nLoss = 1.978113\nVariable usage = 0.00%\n----------Iter = 7900----------\nLoss = 1.976159\nVariable usage = 0.00%\n----------Iter = 7950----------\nLoss = 1.974210\nVariable usage = 0.00%\n----------Iter = 8000----------\nLoss = 1.972265\nVariable usage = 0.00%\n----------Iter = 8050----------\nLoss = 1.970323\nVariable usage = 0.00%\n----------Iter = 8100----------\nLoss = 1.968385\nVariable usage = 0.00%\n----------Iter = 8150----------\nLoss = 1.966451\nVariable usage = 0.00%\n----------Iter = 8200----------\nLoss = 1.964521\nVariable usage = 0.00%\n----------Iter = 8250----------\nLoss = 1.962595\nVariable usage = 0.00%\n----------Iter = 8300----------\nLoss = 1.960673\nVariable usage = 0.00%\n----------Iter = 8350----------\nLoss = 1.958754\nVariable usage = 0.00%\n----------Iter = 8400----------\nLoss = 1.956840\nVariable usage = 0.00%\n----------Iter = 8450----------\nLoss = 1.954929\nVariable usage = 0.00%\n----------Iter = 8500----------\nLoss = 1.953022\nVariable usage = 0.00%\n----------Iter = 8550----------\nLoss = 1.951119\nVariable usage = 0.00%\n----------Iter = 8600----------\nLoss = 1.949220\nVariable usage = 0.00%\n----------Iter = 8650----------\nLoss = 1.947324\nVariable usage = 0.00%\n----------Iter = 8700----------\nLoss = 1.945432\nVariable usage = 0.00%\n----------Iter = 8750----------\nLoss = 1.943544\nVariable usage = 0.00%\n----------Iter = 8800----------\nLoss = 1.941660\nVariable usage = 0.00%\n----------Iter = 8850----------\nLoss = 1.939780\nVariable usage = 0.00%\n----------Iter = 8900----------\nLoss = 1.937903\nVariable usage = 0.00%\n----------Iter = 8950----------\nLoss = 1.936030\nVariable usage = 0.00%\n----------Iter = 9000----------\nLoss = 1.934161\nVariable usage = 0.00%\n----------Iter = 9050----------\nLoss = 1.932295\nVariable usage = 0.00%\n----------Iter = 9100----------\nLoss = 1.930434\nVariable usage = 0.00%\n----------Iter = 9150----------\nLoss = 1.928576\nVariable usage = 0.00%\n----------Iter = 9200----------\nLoss = 1.926721\nVariable usage = 0.00%\n----------Iter = 9250----------\nLoss = 1.924871\nVariable usage = 0.00%\n----------Iter = 9300----------\nLoss = 1.923024\nVariable usage = 0.00%\n----------Iter = 9350----------\nLoss = 1.921180\nVariable usage = 0.00%\n----------Iter = 9400----------\nLoss = 1.919341\nVariable usage = 0.00%\n----------Iter = 9450----------\nLoss = 1.917505\nVariable usage = 0.00%\n----------Iter = 9500----------\nLoss = 1.915673\nVariable usage = 0.00%\n----------Iter = 9550----------\nLoss = 1.913844\nVariable usage = 0.00%\n----------Iter = 9600----------\nLoss = 1.912020\nVariable usage = 0.00%\n----------Iter = 9650----------\nLoss = 1.910198\nVariable usage = 0.00%\n----------Iter = 9700----------\nLoss = 1.908381\nVariable usage = 0.00%\n----------Iter = 9750----------\nLoss = 1.906567\nVariable usage = 0.00%\n----------Iter = 9800----------\nLoss = 1.904756\nVariable usage = 0.00%\n----------Iter = 9850----------\nLoss = 1.902950\nVariable usage = 0.00%\n----------Iter = 9900----------\nLoss = 1.901147\nVariable usage = 0.00%\n----------Iter = 9950----------\nLoss = 1.899347\nVariable usage = 0.00%\n----------Iter = 10000----------\nLoss = 1.897551\nVariable usage = 0.00%\n----------Iter = 10050----------\nLoss = 1.895759\nVariable usage = 0.00%\n----------Iter = 10100----------\nLoss = 1.893970\nVariable usage = 0.00%\n----------Iter = 10150----------\nLoss = 1.892185\nVariable usage = 0.00%\n----------Iter = 10200----------\nLoss = 1.890403\nVariable usage = 0.00%\n----------Iter = 10250----------\nLoss = 1.888625\nVariable usage = 0.00%\n----------Iter = 10300----------\nLoss = 1.886851\nVariable usage = 0.00%\n----------Iter = 10350----------\nLoss = 1.885080\nVariable usage = 0.00%\n----------Iter = 10400----------\nLoss = 1.883313\nVariable usage = 0.00%\n----------Iter = 10450----------\nLoss = 1.881549\nVariable usage = 0.00%\n----------Iter = 10500----------\nLoss = 1.879788\nVariable usage = 0.00%\n----------Iter = 10550----------\nLoss = 1.878032\nVariable usage = 0.00%\n----------Iter = 10600----------\nLoss = 1.876278\nVariable usage = 0.00%\n----------Iter = 10650----------\nLoss = 1.874529\nVariable usage = 0.00%\n----------Iter = 10700----------\nLoss = 1.872782\nVariable usage = 0.00%\n----------Iter = 10750----------\nLoss = 1.871039\nVariable usage = 0.00%\n----------Iter = 10800----------\nLoss = 1.869300\nVariable usage = 0.00%\n----------Iter = 10850----------\nLoss = 1.867564\nVariable usage = 0.00%\n----------Iter = 10900----------\nLoss = 1.865832\nVariable usage = 0.00%\n----------Iter = 10950----------\nLoss = 1.864103\nVariable usage = 0.00%\n----------Iter = 11000----------\nLoss = 1.862377\nVariable usage = 0.00%\n----------Iter = 11050----------\nLoss = 1.860655\nVariable usage = 0.00%\n----------Iter = 11100----------\nLoss = 1.858936\nVariable usage = 0.00%\n----------Iter = 11150----------\nLoss = 1.857221\nVariable usage = 0.00%\n----------Iter = 11200----------\nLoss = 1.855510\nVariable usage = 0.00%\n----------Iter = 11250----------\nLoss = 1.853801\nVariable usage = 0.00%\n----------Iter = 11300----------\nLoss = 1.852096\nVariable usage = 0.00%\n----------Iter = 11350----------\nLoss = 1.850395\nVariable usage = 0.00%\n----------Iter = 11400----------\nLoss = 1.848697\nVariable usage = 0.00%\n----------Iter = 11450----------\nLoss = 1.847002\nVariable usage = 0.00%\n----------Iter = 11500----------\nLoss = 1.845311\nVariable usage = 0.00%\n----------Iter = 11550----------\nLoss = 1.843623\nVariable usage = 0.00%\n----------Iter = 11600----------\nLoss = 1.841938\nVariable usage = 0.00%\n----------Iter = 11650----------\nLoss = 1.840257\nVariable usage = 0.00%\n----------Iter = 11700----------\nLoss = 1.838579\nVariable usage = 0.00%\n----------Iter = 11750----------\nLoss = 1.836905\nVariable usage = 0.00%\n----------Iter = 11800----------\nLoss = 1.835234\nVariable usage = 0.00%\n----------Iter = 11850----------\nLoss = 1.833566\nVariable usage = 0.00%\n----------Iter = 11900----------\nLoss = 1.831901\nVariable usage = 0.00%\n----------Iter = 11950----------\nLoss = 1.830240\nVariable usage = 0.00%\n----------Iter = 12000----------\nLoss = 1.828582\nVariable usage = 0.00%\n----------Iter = 12050----------\nLoss = 1.826928\nVariable usage = 0.00%\n----------Iter = 12100----------\nLoss = 1.825277\nVariable usage = 0.00%\n----------Iter = 12150----------\nLoss = 1.823629\nVariable usage = 0.00%\n----------Iter = 12200----------\nLoss = 1.821984\nVariable usage = 0.00%\n----------Iter = 12250----------\nLoss = 1.820343\nVariable usage = 0.00%\n----------Iter = 12300----------\nLoss = 1.818705\nVariable usage = 0.00%\n----------Iter = 12350----------\nLoss = 1.817070\nVariable usage = 0.00%\n----------Iter = 12400----------\nLoss = 1.815439\nVariable usage = 0.00%\n----------Iter = 12450----------\nLoss = 1.813810\nVariable usage = 0.00%\n----------Iter = 12500----------\nLoss = 1.812185\nVariable usage = 0.00%\n----------Iter = 12550----------\nLoss = 1.810563\nVariable usage = 0.00%\n----------Iter = 12600----------\nLoss = 1.808945\nVariable usage = 0.00%\n----------Iter = 12650----------\nLoss = 1.807330\nVariable usage = 0.00%\n----------Iter = 12700----------\nLoss = 1.805717\nVariable usage = 0.00%\n----------Iter = 12750----------\nLoss = 1.804109\nVariable usage = 0.00%\n----------Iter = 12800----------\nLoss = 1.802503\nVariable usage = 0.00%\n----------Iter = 12850----------\nLoss = 1.800901\nVariable usage = 0.00%\n----------Iter = 12900----------\nLoss = 1.799302\nVariable usage = 0.00%\n----------Iter = 12950----------\nLoss = 1.797706\nVariable usage = 0.00%\n----------Iter = 13000----------\nLoss = 1.796113\nVariable usage = 0.00%\n----------Iter = 13050----------\nLoss = 1.794523\nVariable usage = 0.00%\n----------Iter = 13100----------\nLoss = 1.792937\nVariable usage = 0.00%\n----------Iter = 13150----------\nLoss = 1.791353\nVariable usage = 0.00%\n----------Iter = 13200----------\nLoss = 1.789773\nVariable usage = 0.00%\n----------Iter = 13250----------\nLoss = 1.788196\nVariable usage = 0.00%\n----------Iter = 13300----------\nLoss = 1.786622\nVariable usage = 0.00%\n----------Iter = 13350----------\nLoss = 1.785052\nVariable usage = 0.00%\n----------Iter = 13400----------\nLoss = 1.783484\nVariable usage = 0.00%\n----------Iter = 13450----------\nLoss = 1.781920\nVariable usage = 0.00%\n----------Iter = 13500----------\nLoss = 1.780359\nVariable usage = 0.00%\n----------Iter = 13550----------\nLoss = 1.778800\nVariable usage = 0.00%\n----------Iter = 13600----------\nLoss = 1.777245\nVariable usage = 0.00%\n----------Iter = 13650----------\nLoss = 1.775693\nVariable usage = 0.00%\n----------Iter = 13700----------\nLoss = 1.774144\nVariable usage = 0.00%\n----------Iter = 13750----------\nLoss = 1.772599\nVariable usage = 0.00%\n----------Iter = 13800----------\nLoss = 1.771056\nVariable usage = 0.00%\n----------Iter = 13850----------\nLoss = 1.769517\nVariable usage = 0.00%\n----------Iter = 13900----------\nLoss = 1.767980\nVariable usage = 0.00%\n----------Iter = 13950----------\nLoss = 1.766447\nVariable usage = 0.00%\n----------Iter = 14000----------\nLoss = 1.764916\nVariable usage = 0.00%\n----------Iter = 14050----------\nLoss = 1.763389\nVariable usage = 0.00%\n----------Iter = 14100----------\nLoss = 1.761865\nVariable usage = 0.00%\n----------Iter = 14150----------\nLoss = 1.760344\nVariable usage = 0.00%\n----------Iter = 14200----------\nLoss = 1.758825\nVariable usage = 0.00%\n----------Iter = 14250----------\nLoss = 1.757310\nVariable usage = 0.00%\n----------Iter = 14300----------\nLoss = 1.755798\nVariable usage = 0.00%\n----------Iter = 14350----------\nLoss = 1.754289\nVariable usage = 0.00%\n----------Iter = 14400----------\nLoss = 1.752783\nVariable usage = 0.00%\n----------Iter = 14450----------\nLoss = 1.751280\nVariable usage = 0.00%\n----------Iter = 14500----------\nLoss = 1.749780\nVariable usage = 0.00%\n----------Iter = 14550----------\nLoss = 1.748283\nVariable usage = 0.00%\n----------Iter = 14600----------\nLoss = 1.746789\nVariable usage = 0.00%\n----------Iter = 14650----------\nLoss = 1.745298\nVariable usage = 0.00%\n----------Iter = 14700----------\nLoss = 1.743810\nVariable usage = 0.00%\n----------Iter = 14750----------\nLoss = 1.742324\nVariable usage = 0.00%\n----------Iter = 14800----------\nLoss = 1.740842\nVariable usage = 0.00%\n----------Iter = 14850----------\nLoss = 1.739363\nVariable usage = 0.00%\n----------Iter = 14900----------\nLoss = 1.737887\nVariable usage = 0.00%\n----------Iter = 14950----------\nLoss = 1.736413\nVariable usage = 0.00%\n----------Iter = 15000----------\nLoss = 1.734943\nVariable usage = 0.00%\n----------Iter = 15050----------\nLoss = 1.733476\nVariable usage = 0.00%\n----------Iter = 15100----------\nLoss = 1.732011\nVariable usage = 0.00%\n----------Iter = 15150----------\nLoss = 1.730550\nVariable usage = 0.00%\n----------Iter = 15200----------\nLoss = 1.729091\nVariable usage = 0.00%\n----------Iter = 15250----------\nLoss = 1.727635\nVariable usage = 0.00%\n----------Iter = 15300----------\nLoss = 1.726182\nVariable usage = 0.00%\n----------Iter = 15350----------\nLoss = 1.724733\nVariable usage = 0.00%\n----------Iter = 15400----------\nLoss = 1.723285\nVariable usage = 0.00%\n----------Iter = 15450----------\nLoss = 1.721841\nVariable usage = 0.00%\n----------Iter = 15500----------\nLoss = 1.720400\nVariable usage = 0.00%\n----------Iter = 15550----------\nLoss = 1.718962\nVariable usage = 0.00%\n----------Iter = 15600----------\nLoss = 1.717526\nVariable usage = 0.00%\n----------Iter = 15650----------\nLoss = 1.716094\nVariable usage = 0.00%\n----------Iter = 15700----------\nLoss = 1.714664\nVariable usage = 0.00%\n----------Iter = 15750----------\nLoss = 1.713237\nVariable usage = 0.00%\n----------Iter = 15800----------\nLoss = 1.711813\nVariable usage = 0.00%\n----------Iter = 15850----------\nLoss = 1.710392\nVariable usage = 0.00%\n----------Iter = 15900----------\nLoss = 1.708973\nVariable usage = 0.00%\n----------Iter = 15950----------\nLoss = 1.707558\nVariable usage = 0.00%\n----------Iter = 16000----------\nLoss = 1.706145\nVariable usage = 0.00%\n----------Iter = 16050----------\nLoss = 1.704735\nVariable usage = 0.00%\n----------Iter = 16100----------\nLoss = 1.703328\nVariable usage = 0.00%\n----------Iter = 16150----------\nLoss = 1.701924\nVariable usage = 0.00%\n----------Iter = 16200----------\nLoss = 1.700522\nVariable usage = 0.00%\n----------Iter = 16250----------\nLoss = 1.699124\nVariable usage = 0.00%\n----------Iter = 16300----------\nLoss = 1.697728\nVariable usage = 0.00%\n----------Iter = 16350----------\nLoss = 1.696335\nVariable usage = 0.00%\n----------Iter = 16400----------\nLoss = 1.694945\nVariable usage = 0.00%\n----------Iter = 16450----------\nLoss = 1.693557\nVariable usage = 0.00%\n----------Iter = 16500----------\nLoss = 1.692172\nVariable usage = 0.00%\n----------Iter = 16550----------\nLoss = 1.690791\nVariable usage = 0.00%\n----------Iter = 16600----------\nLoss = 1.689411\nVariable usage = 0.00%\n----------Iter = 16650----------\nLoss = 1.688035\nVariable usage = 0.00%\n----------Iter = 16700----------\nLoss = 1.686661\nVariable usage = 0.00%\n----------Iter = 16750----------\nLoss = 1.685290\nVariable usage = 0.00%\n----------Iter = 16800----------\nLoss = 1.683922\nVariable usage = 0.00%\n----------Iter = 16850----------\nLoss = 1.682557\nVariable usage = 0.00%\n----------Iter = 16900----------\nLoss = 1.681194\nVariable usage = 0.00%\n----------Iter = 16950----------\nLoss = 1.679834\nVariable usage = 0.00%\n----------Iter = 17000----------\nLoss = 1.678477\nVariable usage = 0.00%\n----------Iter = 17050----------\nLoss = 1.677122\nVariable usage = 0.00%\n----------Iter = 17100----------\nLoss = 1.675770\nVariable usage = 0.00%\n----------Iter = 17150----------\nLoss = 1.674421\nVariable usage = 0.00%\n----------Iter = 17200----------\nLoss = 1.673074\nVariable usage = 0.00%\n----------Iter = 17250----------\nLoss = 1.671730\nVariable usage = 0.00%\n----------Iter = 17300----------\nLoss = 1.670389\nVariable usage = 0.00%\n----------Iter = 17350----------\nLoss = 1.669051\nVariable usage = 0.00%\n----------Iter = 17400----------\nLoss = 1.667715\nVariable usage = 0.00%\n----------Iter = 17450----------\nLoss = 1.666382\nVariable usage = 0.00%\n----------Iter = 17500----------\nLoss = 1.665052\nVariable usage = 0.00%\n----------Iter = 17550----------\nLoss = 1.663724\nVariable usage = 0.00%\n----------Iter = 17600----------\nLoss = 1.662399\nVariable usage = 0.00%\n----------Iter = 17650----------\nLoss = 1.661076\nVariable usage = 0.00%\n----------Iter = 17700----------\nLoss = 1.659757\nVariable usage = 0.00%\n----------Iter = 17750----------\nLoss = 1.658439\nVariable usage = 0.00%\n----------Iter = 17800----------\nLoss = 1.657125\nVariable usage = 0.00%\n----------Iter = 17850----------\nLoss = 1.655813\nVariable usage = 0.00%\n----------Iter = 17900----------\nLoss = 1.654503\nVariable usage = 0.00%\n----------Iter = 17950----------\nLoss = 1.653197\nVariable usage = 0.00%\n----------Iter = 18000----------\nLoss = 1.651893\nVariable usage = 0.00%\n----------Iter = 18050----------\nLoss = 1.650591\nVariable usage = 0.00%\n----------Iter = 18100----------\nLoss = 1.649292\nVariable usage = 0.00%\n----------Iter = 18150----------\nLoss = 1.647996\nVariable usage = 0.00%\n----------Iter = 18200----------\nLoss = 1.646703\nVariable usage = 0.00%\n----------Iter = 18250----------\nLoss = 1.645411\nVariable usage = 0.00%\n----------Iter = 18300----------\nLoss = 1.644123\nVariable usage = 0.00%\n----------Iter = 18350----------\nLoss = 1.642837\nVariable usage = 0.00%\n----------Iter = 18400----------\nLoss = 1.641553\nVariable usage = 0.00%\n----------Iter = 18450----------\nLoss = 1.640272\nVariable usage = 0.00%\n----------Iter = 18500----------\nLoss = 1.638994\nVariable usage = 0.00%\n----------Iter = 18550----------\nLoss = 1.637719\nVariable usage = 0.00%\n----------Iter = 18600----------\nLoss = 1.636446\nVariable usage = 0.00%\n----------Iter = 18650----------\nLoss = 1.635175\nVariable usage = 0.00%\n----------Iter = 18700----------\nLoss = 1.633907\nVariable usage = 0.00%\n----------Iter = 18750----------\nLoss = 1.632641\nVariable usage = 0.00%\n----------Iter = 18800----------\nLoss = 1.631378\nVariable usage = 0.00%\n----------Iter = 18850----------\nLoss = 1.630118\nVariable usage = 0.00%\n----------Iter = 18900----------\nLoss = 1.628860\nVariable usage = 0.00%\n----------Iter = 18950----------\nLoss = 1.627604\nVariable usage = 0.00%\n----------Iter = 19000----------\nLoss = 1.626351\nVariable usage = 0.00%\n----------Iter = 19050----------\nLoss = 1.625101\nVariable usage = 0.00%\n----------Iter = 19100----------\nLoss = 1.623853\nVariable usage = 0.00%\n----------Iter = 19150----------\nLoss = 1.622608\nVariable usage = 0.00%\n----------Iter = 19200----------\nLoss = 1.621365\nVariable usage = 0.00%\n----------Iter = 19250----------\nLoss = 1.620124\nVariable usage = 0.00%\n----------Iter = 19300----------\nLoss = 1.618886\nVariable usage = 0.00%\n----------Iter = 19350----------\nLoss = 1.617651\nVariable usage = 0.00%\n----------Iter = 19600----------\nLoss = 1.611510\nVariable usage = 0.00%\n----------Iter = 19650----------\nLoss = 1.610289\nVariable usage = 0.00%\n----------Iter = 19700----------\nLoss = 1.609071\nVariable usage = 0.00%\n----------Iter = 19750----------\nLoss = 1.607855\nVariable usage = 0.00%\n----------Iter = 19800----------\nLoss = 1.606641\nVariable usage = 0.00%\n----------Iter = 19850----------\nLoss = 1.605430\nVariable usage = 0.00%\n----------Iter = 19950----------\nLoss = 1.603015\nVariable usage = 0.00%\n----------Iter = 20000----------\nLoss = 1.601812\nVariable usage = 0.00%\n\n\n- 이부분에서 데이터가 cuda 의 텐서여서 오류가 발생해 cpu, numpy로 변환\n\ntrain_loss_np = np.array([loss.cpu().item() for loss in train_loss_list])\n\n\n# Loss function plot\nplt.figure(figsize=(8, 5))\nplt.plot(50 * np.arange(len(train_loss_np)), train_loss_np)\nplt.title('cLSTM training')\nplt.ylabel('Loss')\nplt.xlabel('Training steps')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 5개의 기상 변수 (기온, 강수량, 풍속, 습도, 일사)에 대한 GC 행렬\nGC = np.array([[1, 1, 0, 0, 0],  # 기온 -&gt; 강수량\n               [0, 1, 1, 0, 0],  # 강수량 -&gt; 풍속\n               [0, 0, 1, 1, 0],  # 풍속 -&gt; 습도\n               [0, 0, 0, 1, 1],  # 습도 -&gt; 일사\n               [0, 0, 0, 0, 1]])  # 일사 -&gt; 기온 (self loop)\n\n\n# Check learned Granger causality\nGC_est = clstm.GC().cpu().data.numpy()\n\nprint('True variable usage = %.2f%%' % (100 * np.mean(GC)))\nprint('Estimated variable usage = %.2f%%' % (100 * np.mean(GC_est)))\nprint('Accuracy = %.2f%%' % (100 * np.mean(GC == GC_est)))\n\n# Make figures\nfig, axarr = plt.subplots(1, 2, figsize=(10, 5))\naxarr[0].imshow(GC, cmap='Blues')\naxarr[0].set_title('GC actual')\naxarr[0].set_ylabel('Affected series')\naxarr[0].set_xlabel('Causal series')\naxarr[0].set_xticks([])\naxarr[0].set_yticks([])\n\naxarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\naxarr[1].set_ylabel('Affected series')\naxarr[1].set_xlabel('Causal series')\naxarr[1].set_xticks([])\naxarr[1].set_yticks([])\n\n# Mark disagreements\nfor i in range(len(GC_est)):\n    for j in range(len(GC_est)):\n        if GC[i, j] != GC_est[i, j]:\n            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n            axarr[1].add_patch(rect)\n\nplt.show()\n\nTrue variable usage = 36.00%\nEstimated variable usage = 0.00%\nAccuracy = 64.00%\n\n\n\n\n\n\n\n\n\n- Granger Causality\n\n순서 : (기온 강수량 풍속 습도 일사)\nGC 실제 변수 간의 인과 관계, GC_est 학습된 모델이 추정한 인과 관계\n실제 GC와 추정된 GC_est를 비교하여 정확도(accuracy) 계산\n불일치하는 인과관계는 빨간색으로 표시\n실제 GC를 어떻게 설정해야하지…..\n왜이렇게 많이 틀렸지?? 뭔가 잘못 됐나"
  },
  {
    "objectID": "posts/7.cRNN.html",
    "href": "posts/7.cRNN.html",
    "title": "7. cRNN",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom synthetic import simulate_lorenz_96\nfrom models.crnn import cRNN, train_model_ista\n\n\nimport pandas as pd\n\n\n# For GPU acceleration\ndevice = torch.device('cuda')\n\n\n# Simulate data\n#X_np, GC = simulate_lorenz_96(p=10, F=10, T=1000)\n#X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\nwt = pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nX_np = wt.iloc[:, 1:].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_np)\n\nX_np = X_scaled.astype(np.float32)\nX = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)\n\n\n# Set up model\ncrnn = cRNN(X.shape[-1], hidden=100).cuda(device=device)\n\n\n# Train with ISTA\ntrain_loss_list = train_model_ista(\n    crnn, X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n    check_every=50)\n\n----------Iter = 50----------\nLoss = 5.275791\nVariable usage = 100.00%\n----------Iter = 100----------\nLoss = 1.339803\nVariable usage = 0.00%\n----------Iter = 150----------\nLoss = 1.338235\nVariable usage = 0.00%\n----------Iter = 200----------\nLoss = 1.336950\nVariable usage = 0.00%\n----------Iter = 250----------\nLoss = 1.335857\nVariable usage = 0.00%\n----------Iter = 300----------\nLoss = 1.334898\nVariable usage = 0.00%\n----------Iter = 350----------\nLoss = 1.334030\nVariable usage = 0.00%\n----------Iter = 400----------\nLoss = 1.333226\nVariable usage = 0.00%\n----------Iter = 450----------\nLoss = 1.332466\nVariable usage = 0.00%\n----------Iter = 500----------\nLoss = 1.331737\nVariable usage = 0.00%\n----------Iter = 550----------\nLoss = 1.331030\nVariable usage = 0.00%\n----------Iter = 600----------\nLoss = 1.330338\nVariable usage = 0.00%\n----------Iter = 650----------\nLoss = 1.329658\nVariable usage = 0.00%\n----------Iter = 700----------\nLoss = 1.328985\nVariable usage = 0.00%\n----------Iter = 750----------\nLoss = 1.328318\nVariable usage = 0.00%\n----------Iter = 800----------\nLoss = 1.327656\nVariable usage = 0.00%\n----------Iter = 850----------\nLoss = 1.326997\nVariable usage = 0.00%\n----------Iter = 900----------\nLoss = 1.326342\nVariable usage = 0.00%\n----------Iter = 950----------\nLoss = 1.325688\nVariable usage = 0.00%\n----------Iter = 1000----------\nLoss = 1.325037\nVariable usage = 0.00%\n----------Iter = 1050----------\nLoss = 1.324387\nVariable usage = 0.00%\n----------Iter = 1100----------\nLoss = 1.323739\nVariable usage = 0.00%\n----------Iter = 1150----------\nLoss = 1.323092\nVariable usage = 0.00%\n----------Iter = 1200----------\nLoss = 1.322447\nVariable usage = 0.00%\n----------Iter = 1250----------\nLoss = 1.321804\nVariable usage = 0.00%\n----------Iter = 1300----------\nLoss = 1.321161\nVariable usage = 0.00%\n----------Iter = 1350----------\nLoss = 1.320521\nVariable usage = 0.00%\n----------Iter = 1400----------\nLoss = 1.319881\nVariable usage = 0.00%\n----------Iter = 1450----------\nLoss = 1.319243\nVariable usage = 0.00%\n----------Iter = 1500----------\nLoss = 1.318606\nVariable usage = 0.00%\n----------Iter = 1550----------\nLoss = 1.317970\nVariable usage = 0.00%\n----------Iter = 1600----------\nLoss = 1.317335\nVariable usage = 0.00%\n----------Iter = 1650----------\nLoss = 1.316702\nVariable usage = 0.00%\n----------Iter = 1700----------\nLoss = 1.316070\nVariable usage = 0.00%\n----------Iter = 1750----------\nLoss = 1.315440\nVariable usage = 0.00%\n----------Iter = 1800----------\nLoss = 1.314810\nVariable usage = 0.00%\n----------Iter = 1850----------\nLoss = 1.314182\nVariable usage = 0.00%\n----------Iter = 1900----------\nLoss = 1.313555\nVariable usage = 0.00%\n----------Iter = 1950----------\nLoss = 1.312930\nVariable usage = 0.00%\n----------Iter = 2000----------\nLoss = 1.312305\nVariable usage = 0.00%\n----------Iter = 2050----------\nLoss = 1.311682\nVariable usage = 0.00%\n----------Iter = 2100----------\nLoss = 1.311061\nVariable usage = 0.00%\n----------Iter = 2150----------\nLoss = 1.310440\nVariable usage = 0.00%\n----------Iter = 2200----------\nLoss = 1.309821\nVariable usage = 0.00%\n----------Iter = 2250----------\nLoss = 1.309202\nVariable usage = 0.00%\n----------Iter = 2300----------\nLoss = 1.308585\nVariable usage = 0.00%\n----------Iter = 2350----------\nLoss = 1.307970\nVariable usage = 0.00%\n----------Iter = 2400----------\nLoss = 1.307355\nVariable usage = 0.00%\n----------Iter = 2450----------\nLoss = 1.306742\nVariable usage = 0.00%\n----------Iter = 2500----------\nLoss = 1.306130\nVariable usage = 0.00%\n----------Iter = 2550----------\nLoss = 1.305519\nVariable usage = 0.00%\n----------Iter = 2600----------\nLoss = 1.304910\nVariable usage = 0.00%\n----------Iter = 2650----------\nLoss = 1.304302\nVariable usage = 0.00%\n----------Iter = 2700----------\nLoss = 1.303695\nVariable usage = 0.00%\n----------Iter = 2750----------\nLoss = 1.303089\nVariable usage = 0.00%\n----------Iter = 2800----------\nLoss = 1.302484\nVariable usage = 0.00%\n----------Iter = 2850----------\nLoss = 1.301881\nVariable usage = 0.00%\n----------Iter = 2900----------\nLoss = 1.301278\nVariable usage = 0.00%\n----------Iter = 2950----------\nLoss = 1.300677\nVariable usage = 0.00%\n----------Iter = 3000----------\nLoss = 1.300077\nVariable usage = 0.00%\n----------Iter = 3050----------\nLoss = 1.299479\nVariable usage = 0.00%\n----------Iter = 3100----------\nLoss = 1.298881\nVariable usage = 0.00%\n----------Iter = 3150----------\nLoss = 1.298285\nVariable usage = 0.00%\n----------Iter = 3200----------\nLoss = 1.297690\nVariable usage = 0.00%\n----------Iter = 3250----------\nLoss = 1.297096\nVariable usage = 0.00%\n----------Iter = 3300----------\nLoss = 1.296504\nVariable usage = 0.00%\n----------Iter = 3350----------\nLoss = 1.295912\nVariable usage = 0.00%\n----------Iter = 3400----------\nLoss = 1.295322\nVariable usage = 0.00%\n----------Iter = 3450----------\nLoss = 1.294733\nVariable usage = 0.00%\n----------Iter = 3500----------\nLoss = 1.294145\nVariable usage = 0.00%\n----------Iter = 3550----------\nLoss = 1.293558\nVariable usage = 0.00%\n----------Iter = 3600----------\nLoss = 1.292972\nVariable usage = 0.00%\n----------Iter = 3650----------\nLoss = 1.292388\nVariable usage = 0.00%\n----------Iter = 3700----------\nLoss = 1.291805\nVariable usage = 0.00%\n----------Iter = 3750----------\nLoss = 1.291223\nVariable usage = 0.00%\n----------Iter = 3800----------\nLoss = 1.290642\nVariable usage = 0.00%\n----------Iter = 3850----------\nLoss = 1.290062\nVariable usage = 0.00%\n----------Iter = 3900----------\nLoss = 1.289483\nVariable usage = 0.00%\n----------Iter = 3950----------\nLoss = 1.288906\nVariable usage = 0.00%\n----------Iter = 4000----------\nLoss = 1.288329\nVariable usage = 0.00%\n----------Iter = 4050----------\nLoss = 1.287754\nVariable usage = 0.00%\n----------Iter = 4100----------\nLoss = 1.287180\nVariable usage = 0.00%\n----------Iter = 4150----------\nLoss = 1.286608\nVariable usage = 0.00%\n----------Iter = 4200----------\nLoss = 1.286036\nVariable usage = 0.00%\n----------Iter = 4250----------\nLoss = 1.285465\nVariable usage = 0.00%\n----------Iter = 4300----------\nLoss = 1.284896\nVariable usage = 0.00%\n----------Iter = 4350----------\nLoss = 1.284328\nVariable usage = 0.00%\n----------Iter = 4400----------\nLoss = 1.283761\nVariable usage = 0.00%\n----------Iter = 4450----------\nLoss = 1.283194\nVariable usage = 0.00%\n----------Iter = 4500----------\nLoss = 1.282630\nVariable usage = 0.00%\n----------Iter = 4550----------\nLoss = 1.282066\nVariable usage = 0.00%\n----------Iter = 4600----------\nLoss = 1.281503\nVariable usage = 0.00%\n----------Iter = 4650----------\nLoss = 1.280942\nVariable usage = 0.00%\n----------Iter = 4700----------\nLoss = 1.280381\nVariable usage = 0.00%\n----------Iter = 4750----------\nLoss = 1.279822\nVariable usage = 0.00%\n----------Iter = 4800----------\nLoss = 1.279264\nVariable usage = 0.00%\n----------Iter = 4850----------\nLoss = 1.278707\nVariable usage = 0.00%\n----------Iter = 4900----------\nLoss = 1.278151\nVariable usage = 0.00%\n----------Iter = 4950----------\nLoss = 1.277596\nVariable usage = 0.00%\n----------Iter = 5000----------\nLoss = 1.277043\nVariable usage = 0.00%\n----------Iter = 5050----------\nLoss = 1.276490\nVariable usage = 0.00%\n----------Iter = 5100----------\nLoss = 1.275938\nVariable usage = 0.00%\n----------Iter = 5150----------\nLoss = 1.275388\nVariable usage = 0.00%\n----------Iter = 5200----------\nLoss = 1.274839\nVariable usage = 0.00%\n----------Iter = 5250----------\nLoss = 1.274291\nVariable usage = 0.00%\n----------Iter = 5300----------\nLoss = 1.273744\nVariable usage = 0.00%\n----------Iter = 5350----------\nLoss = 1.273198\nVariable usage = 0.00%\n----------Iter = 5400----------\nLoss = 1.272653\nVariable usage = 0.00%\n----------Iter = 5450----------\nLoss = 1.272109\nVariable usage = 0.00%\n----------Iter = 5500----------\nLoss = 1.271566\nVariable usage = 0.00%\n----------Iter = 5550----------\nLoss = 1.271025\nVariable usage = 0.00%\n----------Iter = 5600----------\nLoss = 1.270484\nVariable usage = 0.00%\n----------Iter = 5650----------\nLoss = 1.269945\nVariable usage = 0.00%\n----------Iter = 5700----------\nLoss = 1.269406\nVariable usage = 0.00%\n----------Iter = 5750----------\nLoss = 1.268869\nVariable usage = 0.00%\n----------Iter = 5800----------\nLoss = 1.268332\nVariable usage = 0.00%\n----------Iter = 5850----------\nLoss = 1.267797\nVariable usage = 0.00%\n----------Iter = 5900----------\nLoss = 1.267263\nVariable usage = 0.00%\n----------Iter = 5950----------\nLoss = 1.266730\nVariable usage = 0.00%\n----------Iter = 6000----------\nLoss = 1.266199\nVariable usage = 0.00%\n----------Iter = 6050----------\nLoss = 1.265668\nVariable usage = 0.00%\n----------Iter = 6100----------\nLoss = 1.265138\nVariable usage = 0.00%\n----------Iter = 6150----------\nLoss = 1.264609\nVariable usage = 0.00%\n----------Iter = 6200----------\nLoss = 1.264081\nVariable usage = 0.00%\n----------Iter = 6250----------\nLoss = 1.263555\nVariable usage = 0.00%\n----------Iter = 6300----------\nLoss = 1.263029\nVariable usage = 0.00%\n----------Iter = 6350----------\nLoss = 1.262504\nVariable usage = 0.00%\n----------Iter = 6400----------\nLoss = 1.261981\nVariable usage = 0.00%\n----------Iter = 6450----------\nLoss = 1.261458\nVariable usage = 0.00%\n----------Iter = 6500----------\nLoss = 1.260937\nVariable usage = 0.00%\n----------Iter = 6550----------\nLoss = 1.260416\nVariable usage = 0.00%\n----------Iter = 6600----------\nLoss = 1.259897\nVariable usage = 0.00%\n----------Iter = 6650----------\nLoss = 1.259379\nVariable usage = 0.00%\n----------Iter = 6700----------\nLoss = 1.258862\nVariable usage = 0.00%\n----------Iter = 6750----------\nLoss = 1.258345\nVariable usage = 0.00%\n----------Iter = 6800----------\nLoss = 1.257830\nVariable usage = 0.00%\n----------Iter = 6850----------\nLoss = 1.257316\nVariable usage = 0.00%\n----------Iter = 6900----------\nLoss = 1.256803\nVariable usage = 0.00%\n----------Iter = 6950----------\nLoss = 1.256291\nVariable usage = 0.00%\n----------Iter = 7000----------\nLoss = 1.255780\nVariable usage = 0.00%\n----------Iter = 7050----------\nLoss = 1.255270\nVariable usage = 0.00%\n----------Iter = 7100----------\nLoss = 1.254761\nVariable usage = 0.00%\n----------Iter = 7150----------\nLoss = 1.254253\nVariable usage = 0.00%\n----------Iter = 7200----------\nLoss = 1.253746\nVariable usage = 0.00%\n----------Iter = 7250----------\nLoss = 1.253240\nVariable usage = 0.00%\n----------Iter = 7300----------\nLoss = 1.252735\nVariable usage = 0.00%\n----------Iter = 7350----------\nLoss = 1.252231\nVariable usage = 0.00%\n----------Iter = 7400----------\nLoss = 1.251728\nVariable usage = 0.00%\n----------Iter = 7450----------\nLoss = 1.251226\nVariable usage = 0.00%\n----------Iter = 7500----------\nLoss = 1.250725\nVariable usage = 0.00%\n----------Iter = 7550----------\nLoss = 1.250225\nVariable usage = 0.00%\n----------Iter = 7600----------\nLoss = 1.249726\nVariable usage = 0.00%\n----------Iter = 7650----------\nLoss = 1.249228\nVariable usage = 0.00%\n----------Iter = 7700----------\nLoss = 1.248731\nVariable usage = 0.00%\n----------Iter = 7750----------\nLoss = 1.248235\nVariable usage = 0.00%\n----------Iter = 7800----------\nLoss = 1.247740\nVariable usage = 0.00%\n----------Iter = 7850----------\nLoss = 1.247246\nVariable usage = 0.00%\n----------Iter = 7900----------\nLoss = 1.246753\nVariable usage = 0.00%\n----------Iter = 7950----------\nLoss = 1.246261\nVariable usage = 0.00%\n----------Iter = 8000----------\nLoss = 1.245770\nVariable usage = 0.00%\n----------Iter = 8050----------\nLoss = 1.245280\nVariable usage = 0.00%\n----------Iter = 8100----------\nLoss = 1.244790\nVariable usage = 0.00%\n----------Iter = 8150----------\nLoss = 1.244302\nVariable usage = 0.00%\n----------Iter = 8200----------\nLoss = 1.243815\nVariable usage = 0.00%\n----------Iter = 8250----------\nLoss = 1.243329\nVariable usage = 0.00%\n----------Iter = 8300----------\nLoss = 1.242844\nVariable usage = 0.00%\n----------Iter = 8350----------\nLoss = 1.242360\nVariable usage = 0.00%\n----------Iter = 8400----------\nLoss = 1.241876\nVariable usage = 0.00%\n----------Iter = 8450----------\nLoss = 1.241394\nVariable usage = 0.00%\n----------Iter = 8500----------\nLoss = 1.240913\nVariable usage = 0.00%\n----------Iter = 8550----------\nLoss = 1.240433\nVariable usage = 0.00%\n----------Iter = 8600----------\nLoss = 1.239953\nVariable usage = 0.00%\n----------Iter = 8650----------\nLoss = 1.239475\nVariable usage = 0.00%\n----------Iter = 8700----------\nLoss = 1.238997\nVariable usage = 0.00%\n----------Iter = 8750----------\nLoss = 1.238521\nVariable usage = 0.00%\n----------Iter = 8800----------\nLoss = 1.238045\nVariable usage = 0.00%\n----------Iter = 8850----------\nLoss = 1.237571\nVariable usage = 0.00%\n----------Iter = 8900----------\nLoss = 1.237097\nVariable usage = 0.00%\n----------Iter = 8950----------\nLoss = 1.236624\nVariable usage = 0.00%\n----------Iter = 9000----------\nLoss = 1.236152\nVariable usage = 0.00%\n----------Iter = 9050----------\nLoss = 1.235682\nVariable usage = 0.00%\n----------Iter = 9100----------\nLoss = 1.235212\nVariable usage = 0.00%\n----------Iter = 9150----------\nLoss = 1.234743\nVariable usage = 0.00%\n----------Iter = 9200----------\nLoss = 1.234275\nVariable usage = 0.00%\n----------Iter = 9250----------\nLoss = 1.233808\nVariable usage = 0.00%\n----------Iter = 9300----------\nLoss = 1.233341\nVariable usage = 0.00%\n----------Iter = 9350----------\nLoss = 1.232876\nVariable usage = 0.00%\n----------Iter = 9400----------\nLoss = 1.232412\nVariable usage = 0.00%\n----------Iter = 9450----------\nLoss = 1.231948\nVariable usage = 0.00%\n----------Iter = 9500----------\nLoss = 1.231486\nVariable usage = 0.00%\n----------Iter = 9550----------\nLoss = 1.231025\nVariable usage = 0.00%\n----------Iter = 9600----------\nLoss = 1.230564\nVariable usage = 0.00%\n----------Iter = 9650----------\nLoss = 1.230104\nVariable usage = 0.00%\n----------Iter = 9700----------\nLoss = 1.229646\nVariable usage = 0.00%\n----------Iter = 9750----------\nLoss = 1.229188\nVariable usage = 0.00%\n----------Iter = 9800----------\nLoss = 1.228731\nVariable usage = 0.00%\n----------Iter = 9850----------\nLoss = 1.228275\nVariable usage = 0.00%\n----------Iter = 9900----------\nLoss = 1.227820\nVariable usage = 0.00%\n----------Iter = 9950----------\nLoss = 1.227366\nVariable usage = 0.00%\n----------Iter = 10000----------\nLoss = 1.226912\nVariable usage = 0.00%\n----------Iter = 10050----------\nLoss = 1.226460\nVariable usage = 0.00%\n----------Iter = 10100----------\nLoss = 1.226008\nVariable usage = 0.00%\n----------Iter = 10150----------\nLoss = 1.225558\nVariable usage = 0.00%\n----------Iter = 10200----------\nLoss = 1.225108\nVariable usage = 0.00%\n----------Iter = 10250----------\nLoss = 1.224659\nVariable usage = 0.00%\n----------Iter = 10300----------\nLoss = 1.224212\nVariable usage = 0.00%\n----------Iter = 10350----------\nLoss = 1.223765\nVariable usage = 0.00%\n----------Iter = 10400----------\nLoss = 1.223318\nVariable usage = 0.00%\n----------Iter = 10450----------\nLoss = 1.222873\nVariable usage = 0.00%\n----------Iter = 10500----------\nLoss = 1.222429\nVariable usage = 0.00%\n----------Iter = 10550----------\nLoss = 1.221986\nVariable usage = 0.00%\n----------Iter = 10600----------\nLoss = 1.221543\nVariable usage = 0.00%\n----------Iter = 10650----------\nLoss = 1.221102\nVariable usage = 0.00%\n----------Iter = 10700----------\nLoss = 1.220661\nVariable usage = 0.00%\n----------Iter = 10750----------\nLoss = 1.220221\nVariable usage = 0.00%\n----------Iter = 10800----------\nLoss = 1.219782\nVariable usage = 0.00%\n----------Iter = 10850----------\nLoss = 1.219344\nVariable usage = 0.00%\n----------Iter = 10900----------\nLoss = 1.218906\nVariable usage = 0.00%\n----------Iter = 10950----------\nLoss = 1.218470\nVariable usage = 0.00%\n----------Iter = 11000----------\nLoss = 1.218035\nVariable usage = 0.00%\n----------Iter = 11050----------\nLoss = 1.217600\nVariable usage = 0.00%\n----------Iter = 11100----------\nLoss = 1.217166\nVariable usage = 0.00%\n----------Iter = 11150----------\nLoss = 1.216733\nVariable usage = 0.00%\n----------Iter = 11200----------\nLoss = 1.216301\nVariable usage = 0.00%\n----------Iter = 11250----------\nLoss = 1.215870\nVariable usage = 0.00%\n----------Iter = 11300----------\nLoss = 1.215440\nVariable usage = 0.00%\n----------Iter = 11350----------\nLoss = 1.215011\nVariable usage = 0.00%\n----------Iter = 11400----------\nLoss = 1.214582\nVariable usage = 0.00%\n----------Iter = 11450----------\nLoss = 1.214154\nVariable usage = 0.00%\n----------Iter = 11500----------\nLoss = 1.213727\nVariable usage = 0.00%\n----------Iter = 11550----------\nLoss = 1.213301\nVariable usage = 0.00%\n----------Iter = 11600----------\nLoss = 1.212876\nVariable usage = 0.00%\n----------Iter = 11650----------\nLoss = 1.212452\nVariable usage = 0.00%\n----------Iter = 11700----------\nLoss = 1.212028\nVariable usage = 0.00%\n----------Iter = 11750----------\nLoss = 1.211606\nVariable usage = 0.00%\n----------Iter = 11800----------\nLoss = 1.211184\nVariable usage = 0.00%\n----------Iter = 11850----------\nLoss = 1.210763\nVariable usage = 0.00%\n----------Iter = 11900----------\nLoss = 1.210343\nVariable usage = 0.00%\n----------Iter = 11950----------\nLoss = 1.209924\nVariable usage = 0.00%\n----------Iter = 12000----------\nLoss = 1.209505\nVariable usage = 0.00%\n----------Iter = 12050----------\nLoss = 1.209087\nVariable usage = 0.00%\n----------Iter = 12100----------\nLoss = 1.208671\nVariable usage = 0.00%\n----------Iter = 12150----------\nLoss = 1.208255\nVariable usage = 0.00%\n----------Iter = 12200----------\nLoss = 1.207840\nVariable usage = 0.00%\n----------Iter = 12250----------\nLoss = 1.207425\nVariable usage = 0.00%\n----------Iter = 12300----------\nLoss = 1.207012\nVariable usage = 0.00%\n----------Iter = 12350----------\nLoss = 1.206599\nVariable usage = 0.00%\n----------Iter = 12400----------\nLoss = 1.206188\nVariable usage = 0.00%\n----------Iter = 12450----------\nLoss = 1.205777\nVariable usage = 0.00%\n----------Iter = 12500----------\nLoss = 1.205367\nVariable usage = 0.00%\n----------Iter = 12550----------\nLoss = 1.204957\nVariable usage = 0.00%\n----------Iter = 12600----------\nLoss = 1.204549\nVariable usage = 0.00%\n----------Iter = 12650----------\nLoss = 1.204141\nVariable usage = 0.00%\n----------Iter = 12700----------\nLoss = 1.203734\nVariable usage = 0.00%\n----------Iter = 12750----------\nLoss = 1.203328\nVariable usage = 0.00%\n----------Iter = 12800----------\nLoss = 1.202923\nVariable usage = 0.00%\n----------Iter = 12850----------\nLoss = 1.202518\nVariable usage = 0.00%\n----------Iter = 12900----------\nLoss = 1.202115\nVariable usage = 0.00%\n----------Iter = 12950----------\nLoss = 1.201712\nVariable usage = 0.00%\n----------Iter = 13000----------\nLoss = 1.201310\nVariable usage = 0.00%\n----------Iter = 13050----------\nLoss = 1.200909\nVariable usage = 0.00%\n----------Iter = 13100----------\nLoss = 1.200508\nVariable usage = 0.00%\n----------Iter = 13150----------\nLoss = 1.200109\nVariable usage = 0.00%\n----------Iter = 13200----------\nLoss = 1.199710\nVariable usage = 0.00%\n----------Iter = 13250----------\nLoss = 1.199312\nVariable usage = 0.00%\n----------Iter = 13300----------\nLoss = 1.198915\nVariable usage = 0.00%\n----------Iter = 13350----------\nLoss = 1.198518\nVariable usage = 0.00%\n----------Iter = 13400----------\nLoss = 1.198123\nVariable usage = 0.00%\n----------Iter = 13450----------\nLoss = 1.197728\nVariable usage = 0.00%\n----------Iter = 13500----------\nLoss = 1.197334\nVariable usage = 0.00%\n----------Iter = 13550----------\nLoss = 1.196941\nVariable usage = 0.00%\n----------Iter = 13600----------\nLoss = 1.196548\nVariable usage = 0.00%\n----------Iter = 13650----------\nLoss = 1.196156\nVariable usage = 0.00%\n----------Iter = 13700----------\nLoss = 1.195765\nVariable usage = 0.00%\n----------Iter = 13750----------\nLoss = 1.195375\nVariable usage = 0.00%\n----------Iter = 13800----------\nLoss = 1.194986\nVariable usage = 0.00%\n----------Iter = 13850----------\nLoss = 1.194597\nVariable usage = 0.00%\n----------Iter = 13900----------\nLoss = 1.194210\nVariable usage = 0.00%\n----------Iter = 13950----------\nLoss = 1.193823\nVariable usage = 0.00%\n----------Iter = 14000----------\nLoss = 1.193436\nVariable usage = 0.00%\n----------Iter = 14050----------\nLoss = 1.193051\nVariable usage = 0.00%\n----------Iter = 14100----------\nLoss = 1.192666\nVariable usage = 0.00%\n----------Iter = 14150----------\nLoss = 1.192282\nVariable usage = 0.00%\n----------Iter = 14200----------\nLoss = 1.191899\nVariable usage = 0.00%\n----------Iter = 14250----------\nLoss = 1.191517\nVariable usage = 0.00%\n----------Iter = 14300----------\nLoss = 1.191135\nVariable usage = 0.00%\n----------Iter = 14350----------\nLoss = 1.190754\nVariable usage = 0.00%\n----------Iter = 14400----------\nLoss = 1.190374\nVariable usage = 0.00%\n----------Iter = 14450----------\nLoss = 1.189995\nVariable usage = 0.00%\n----------Iter = 14500----------\nLoss = 1.189616\nVariable usage = 0.00%\n----------Iter = 14550----------\nLoss = 1.189238\nVariable usage = 0.00%\n----------Iter = 14600----------\nLoss = 1.188861\nVariable usage = 0.00%\n----------Iter = 14650----------\nLoss = 1.188485\nVariable usage = 0.00%\n----------Iter = 14700----------\nLoss = 1.188109\nVariable usage = 0.00%\n----------Iter = 14750----------\nLoss = 1.187734\nVariable usage = 0.00%\n----------Iter = 14800----------\nLoss = 1.187360\nVariable usage = 0.00%\n----------Iter = 14850----------\nLoss = 1.186987\nVariable usage = 0.00%\n----------Iter = 14900----------\nLoss = 1.186615\nVariable usage = 0.00%\n----------Iter = 14950----------\nLoss = 1.186243\nVariable usage = 0.00%\n----------Iter = 15000----------\nLoss = 1.185872\nVariable usage = 0.00%\n----------Iter = 15050----------\nLoss = 1.185501\nVariable usage = 0.00%\n----------Iter = 15100----------\nLoss = 1.185132\nVariable usage = 0.00%\n----------Iter = 15150----------\nLoss = 1.184763\nVariable usage = 0.00%\n----------Iter = 15200----------\nLoss = 1.184395\nVariable usage = 0.00%\n----------Iter = 15250----------\nLoss = 1.184027\nVariable usage = 0.00%\n----------Iter = 15300----------\nLoss = 1.183661\nVariable usage = 0.00%\n----------Iter = 15350----------\nLoss = 1.183295\nVariable usage = 0.00%\n----------Iter = 15400----------\nLoss = 1.182929\nVariable usage = 0.00%\n----------Iter = 15450----------\nLoss = 1.182565\nVariable usage = 0.00%\n----------Iter = 15500----------\nLoss = 1.182201\nVariable usage = 0.00%\n----------Iter = 15550----------\nLoss = 1.181838\nVariable usage = 0.00%\n----------Iter = 15600----------\nLoss = 1.181476\nVariable usage = 0.00%\n----------Iter = 15650----------\nLoss = 1.181114\nVariable usage = 0.00%\n----------Iter = 15700----------\nLoss = 1.180754\nVariable usage = 0.00%\n----------Iter = 15750----------\nLoss = 1.180393\nVariable usage = 0.00%\n----------Iter = 15800----------\nLoss = 1.180034\nVariable usage = 0.00%\n----------Iter = 15850----------\nLoss = 1.179675\nVariable usage = 0.00%\n----------Iter = 15900----------\nLoss = 1.179317\nVariable usage = 0.00%\n----------Iter = 15950----------\nLoss = 1.178960\nVariable usage = 0.00%\n----------Iter = 16000----------\nLoss = 1.178604\nVariable usage = 0.00%\n----------Iter = 16050----------\nLoss = 1.178248\nVariable usage = 0.00%\n----------Iter = 16100----------\nLoss = 1.177893\nVariable usage = 0.00%\n----------Iter = 16150----------\nLoss = 1.177538\nVariable usage = 0.00%\n----------Iter = 16200----------\nLoss = 1.177184\nVariable usage = 0.00%\n----------Iter = 16250----------\nLoss = 1.176831\nVariable usage = 0.00%\n----------Iter = 16300----------\nLoss = 1.176479\nVariable usage = 0.00%\n----------Iter = 16350----------\nLoss = 1.176128\nVariable usage = 0.00%\n----------Iter = 16400----------\nLoss = 1.175777\nVariable usage = 0.00%\n----------Iter = 16450----------\nLoss = 1.175426\nVariable usage = 0.00%\n----------Iter = 16500----------\nLoss = 1.175077\nVariable usage = 0.00%\n----------Iter = 16550----------\nLoss = 1.174728\nVariable usage = 0.00%\n----------Iter = 16600----------\nLoss = 1.174380\nVariable usage = 0.00%\n----------Iter = 16650----------\nLoss = 1.174033\nVariable usage = 0.00%\n----------Iter = 16700----------\nLoss = 1.173686\nVariable usage = 0.00%\n----------Iter = 16750----------\nLoss = 1.173340\nVariable usage = 0.00%\n----------Iter = 16800----------\nLoss = 1.172995\nVariable usage = 0.00%\n----------Iter = 16850----------\nLoss = 1.172650\nVariable usage = 0.00%\n----------Iter = 16900----------\nLoss = 1.172306\nVariable usage = 0.00%\n----------Iter = 16950----------\nLoss = 1.171963\nVariable usage = 0.00%\n----------Iter = 17000----------\nLoss = 1.171620\nVariable usage = 0.00%\n----------Iter = 17050----------\nLoss = 1.171278\nVariable usage = 0.00%\n----------Iter = 17100----------\nLoss = 1.170937\nVariable usage = 0.00%\n----------Iter = 17150----------\nLoss = 1.170597\nVariable usage = 0.00%\n----------Iter = 17200----------\nLoss = 1.170257\nVariable usage = 0.00%\n----------Iter = 17250----------\nLoss = 1.169918\nVariable usage = 0.00%\n----------Iter = 17300----------\nLoss = 1.169579\nVariable usage = 0.00%\n----------Iter = 17350----------\nLoss = 1.169242\nVariable usage = 0.00%\n----------Iter = 17400----------\nLoss = 1.168904\nVariable usage = 0.00%\n----------Iter = 17450----------\nLoss = 1.168568\nVariable usage = 0.00%\n----------Iter = 17500----------\nLoss = 1.168232\nVariable usage = 0.00%\n----------Iter = 17550----------\nLoss = 1.167897\nVariable usage = 0.00%\n----------Iter = 17600----------\nLoss = 1.167563\nVariable usage = 0.00%\n----------Iter = 17650----------\nLoss = 1.167229\nVariable usage = 0.00%\n----------Iter = 17700----------\nLoss = 1.166896\nVariable usage = 0.00%\n----------Iter = 17750----------\nLoss = 1.166563\nVariable usage = 0.00%\n----------Iter = 17800----------\nLoss = 1.166232\nVariable usage = 0.00%\n----------Iter = 17850----------\nLoss = 1.165900\nVariable usage = 0.00%\n----------Iter = 17900----------\nLoss = 1.165570\nVariable usage = 0.00%\n----------Iter = 17950----------\nLoss = 1.165240\nVariable usage = 0.00%\n----------Iter = 18000----------\nLoss = 1.164911\nVariable usage = 0.00%\n----------Iter = 18050----------\nLoss = 1.164583\nVariable usage = 0.00%\n----------Iter = 18100----------\nLoss = 1.164255\nVariable usage = 0.00%\n----------Iter = 18150----------\nLoss = 1.163928\nVariable usage = 0.00%\n----------Iter = 18200----------\nLoss = 1.163601\nVariable usage = 0.00%\n----------Iter = 18250----------\nLoss = 1.163275\nVariable usage = 0.00%\n----------Iter = 18300----------\nLoss = 1.162950\nVariable usage = 0.00%\n----------Iter = 18350----------\nLoss = 1.162626\nVariable usage = 0.00%\n----------Iter = 18400----------\nLoss = 1.162302\nVariable usage = 0.00%\n----------Iter = 18450----------\nLoss = 1.161978\nVariable usage = 0.00%\n----------Iter = 18500----------\nLoss = 1.161656\nVariable usage = 0.00%\n----------Iter = 18550----------\nLoss = 1.161334\nVariable usage = 0.00%\n----------Iter = 18600----------\nLoss = 1.161013\nVariable usage = 0.00%\n----------Iter = 18650----------\nLoss = 1.160692\nVariable usage = 0.00%\n----------Iter = 18700----------\nLoss = 1.160372\nVariable usage = 0.00%\n----------Iter = 18750----------\nLoss = 1.160052\nVariable usage = 0.00%\n----------Iter = 18800----------\nLoss = 1.159734\nVariable usage = 0.00%\n----------Iter = 18850----------\nLoss = 1.159415\nVariable usage = 0.00%\n----------Iter = 18900----------\nLoss = 1.159098\nVariable usage = 0.00%\n----------Iter = 18950----------\nLoss = 1.158781\nVariable usage = 0.00%\n----------Iter = 19000----------\nLoss = 1.158465\nVariable usage = 0.00%\n----------Iter = 19050----------\nLoss = 1.158149\nVariable usage = 0.00%\n----------Iter = 19100----------\nLoss = 1.157834\nVariable usage = 0.00%\n----------Iter = 19150----------\nLoss = 1.157520\nVariable usage = 0.00%\n----------Iter = 19200----------\nLoss = 1.157206\nVariable usage = 0.00%\n----------Iter = 19250----------\nLoss = 1.156893\nVariable usage = 0.00%\n----------Iter = 19300----------\nLoss = 1.156581\nVariable usage = 0.00%\n----------Iter = 19350----------\nLoss = 1.156269\nVariable usage = 0.00%\n----------Iter = 19400----------\nLoss = 1.155958\nVariable usage = 0.00%\n----------Iter = 19450----------\nLoss = 1.155647\nVariable usage = 0.00%\n----------Iter = 19500----------\nLoss = 1.155337\nVariable usage = 0.00%\n----------Iter = 19550----------\nLoss = 1.155028\nVariable usage = 0.00%\n----------Iter = 19600----------\nLoss = 1.154719\nVariable usage = 0.00%\n----------Iter = 19650----------\nLoss = 1.154411\nVariable usage = 0.00%\n----------Iter = 19700----------\nLoss = 1.154104\nVariable usage = 0.00%\n----------Iter = 19750----------\nLoss = 1.153797\nVariable usage = 0.00%\n----------Iter = 19800----------\nLoss = 1.153490\nVariable usage = 0.00%\n----------Iter = 19850----------\nLoss = 1.153185\nVariable usage = 0.00%\n----------Iter = 19900----------\nLoss = 1.152880\nVariable usage = 0.00%\n----------Iter = 19950----------\nLoss = 1.152575\nVariable usage = 0.00%\n----------Iter = 20000----------\nLoss = 1.152271\nVariable usage = 0.00%\n\n\n\ntrain_loss_np = np.array([loss.cpu().item() for loss in train_loss_list])\n\n\n# Loss function plot\nplt.figure(figsize=(8, 5))\nplt.plot(50 * np.arange(len(train_loss_np)), train_loss_np)\nplt.title('cLSTM training')\nplt.ylabel('Loss')\nplt.xlabel('Training steps')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 5개의 기상 변수 (기온, 강수량, 풍속, 습도, 일사)에 대한 GC 행렬\nGC = np.array([[1, 1, 0, 0, 0],  # 기온 -&gt; 강수량\n               [0, 1, 1, 0, 0],  # 강수량 -&gt; 풍속\n               [0, 0, 1, 1, 0],  # 풍속 -&gt; 습도\n               [0, 0, 0, 1, 1],  # 습도 -&gt; 일사\n               [0, 0, 0, 0, 1]])  # 일사 -&gt; 기온 (self loop)\n\n\n# Check learned Granger causality\nGC_est = crnn.GC().cpu().data.numpy()\n\nprint('True variable usage = %.2f%%' % (100 * np.mean(GC)))\nprint('Estimated variable usage = %.2f%%' % (100 * np.mean(GC_est)))\nprint('Accuracy = %.2f%%' % (100 * np.mean(GC == GC_est)))\n\n# Make figures\nfig, axarr = plt.subplots(1, 2, figsize=(10, 5))\naxarr[0].imshow(GC, cmap='Blues')\naxarr[0].set_title('GC actual')\naxarr[0].set_ylabel('Affected series')\naxarr[0].set_xlabel('Causal series')\naxarr[0].set_xticks([])\naxarr[0].set_yticks([])\n\naxarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\naxarr[1].set_ylabel('Affected series')\naxarr[1].set_xlabel('Causal series')\naxarr[1].set_xticks([])\naxarr[1].set_yticks([])\n\n# Mark disagreements\nfor i in range(len(GC_est)):\n    for j in range(len(GC_est)):\n        if GC[i, j] != GC_est[i, j]:\n            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n            axarr[1].add_patch(rect)\n\nplt.show()\n\nTrue variable usage = 36.00%\nEstimated variable usage = 0.00%\nAccuracy = 64.00%"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "poster2025",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 4, 2025\n\n\n7. cRNN\n\n\n이상민 \n\n\n\n\nApr 3, 2025\n\n\n6. cMLP\n\n\n이상민 \n\n\n\n\nApr 2, 2025\n\n\n5. cLSTM\n\n\n이상민 \n\n\n\n\nApr 2, 2025\n\n\n5. cLSTM\n\n\n이상민 \n\n\n\n\nApr 1, 2025\n\n\n4. Neural Granger Causality\n\n\n이상민 \n\n\n\n\nMar 30, 2025\n\n\n3. Granger Causality\n\n\n이상민 \n\n\n\n\nMar 29, 2025\n\n\n2. 시각화 및 VAR 적합\n\n\n이상민 \n\n\n\n\nMar 24, 2025\n\n\n1. 2024 기상데이터 전처리\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/3.GrangerCausality.html",
    "href": "posts/3.GrangerCausality.html",
    "title": "3. Granger Causality",
    "section": "",
    "text": "링크 : Granger Causality"
  },
  {
    "objectID": "posts/3.GrangerCausality.html#granger-causality-a-review-and-recend-advances",
    "href": "posts/3.GrangerCausality.html#granger-causality-a-review-and-recend-advances",
    "title": "3. Granger Causality",
    "section": "Granger Causality : A Review and Recend Advances",
    "text": "Granger Causality : A Review and Recend Advances\n- 다양한 분야에서 시계열 간의 상호작용을 이해하는 것이 중요\n- 우리는 시계열 간의 인과적 상호작용을 이해하고자 하지만 실험이 불가능하고 현상에 대한 기계적 모델도 없는 상황에서 관측 데이터에 기반한 추론만 가능하다는 제한이 존재\n\n(미래를 모르기 때문에)\n\n- 이러한 경우, 시계열에 내재된 시간 순서성을 활용해, 과거가 미래에 영향을 준다는 방향으로 제한된 인과적 설명을 이끌어내는 프레임워크 제안\n- 시계열 \\(y_{t}\\) 의 과거 값이 다른 시계열 \\(x_{t}\\) 의 미래값을 얼마나 잘 예측할 수 있는지에 기반하여 인과성의 개념을 제안\n\n제한된 모델 : $Y_{t}=+ _{i=1}^{p} iY{t-i} + _t $\n\n과거의 Y 값만 사용하여 \\(Y_t\\)를 예측하는 모델\n\n확장된 모델 : $Y_{t}=+ {i=1}^{p} iY{t-i} + {j=1}^{q} Y_jX_{t-j} + _t $\n\n과거의 Y값 뿐만 아니라 X의 과거 값까지 사용하여 예측\n\n\n- 제약\n\nreal-valued time series with\nlinear dynamics dependent on\na knwn number of past lagged observations, with\nobservations available at a fixed, discrete sampling rate that matches the time scale of the causal structure of interest\n\n- “Granger Causality”는 단순한 예측 가능성을 의미\n\nGranger 에서 말하는 인과성은 진정한 인과관계 X\nY의 과거 정보를 포함해서 X의 예측오차가 줄어들면 Y가 X의 Granger causality\n\n\\(H&lt;t\\) 시간 t 이전의 모든 정보 집합\n\\(P(X_t|H&lt;t)\\) 해당 정보 집합을 통한 x의 최적의 예측\n\\(var(x_t-P(x_t|H&lt;t))\\) $ &lt; $ \\(var(x_t-P(x_t|H&lt;t / y&lt;t)\\)\n\n\n- VAR 모델 기반 Granger 모델\n\n\\(A^0x_t=\\sum_{k=1}^{d}A^kX_{t-k}+\\epsilon_{t}\\)\n\n시간 t에서의 변수 벡터를 \\(X_{t} =(x_1t, x_2t, ...x_pt)^T\\)\n\\(A_{k}\\)는 시차(lag) : k에 대한 회귀계수 행렬\n\\(\\epsilon_t\\) : 백색 잡음\n\n\n- Granger 인과성의 기본가정(제한점) &lt;- 분석을 하기위해선 아래가정 충족해야함\n\n연속형 시계열\n선형성\n이산된 시간\n고정된 시차\n정상성\n완전한 관측(오차가 없어야함)\n관련된 모든 변수가 포함되어야함\n\n- 초기 Granger 인과성 검증 방법\n\n이변량 모델\n\n\\(a_{0x}*x_t=\\sum_{k=1}^{d}a^{(k)}_{xx}x_{t-k}+\\sum_{k=1}^{d}a^{(k)}_{xy}y_{t-k}+e_{t,x}\\)\n\\(a_{0y}*y_t=\\sum_{k=1}^{d}a^{(k)}_{yy}y_{t-k}+\\sum_{k=1}^{d}a^{(k)}_{yx}x_{t-k}+e_{t,y}\\)\n\nReduced Model, Full model을 비교해 F검정 실시\n\n\\(F=\\frac{(RSS_{red}-RSS_{full}/(r-s))}{RSS_{full}/(T-r)}\\)\n귀무가설 \\(H_{0} : Y_{t}는 X_{t} granger 원인이 아니다(Y_j=0)\\)\n대립가설 \\(H_{1} : Y_{t}는 X_{t} granger 원인이다(Y_j\\neq0)\\)\n\n\n- 전통적으로 선형 VAR 모델 가정으로 기반\n\n하지만 현실의 시스템은 훨씬 복잡하고 전통적 프레임은 한계점 존재\n\n이변량 모델을 사용하여 고차원 데이터는 정확하 분석 어려움\n비선형성과 비정규성 문제(VAR모델은 이러한 특성 반영 불가)\n불규칙 샘플링 및 관측 데이터 문제(고정된 시간 간격으로 수집, 현실에서는 다양한 주기로 변화 발생)\n\n\n- 해결을 위한 현대의 시도\n\n네트워크 Granger 인과성 : 여러개의 시계열 변수를 동시에 고려하여 다변량 인과성 분석\n비선형, 고차원 데이터 처리 : Lasso 및 Group Lasso 를 활용한 고차원 VAR모델, 딥러닝, 머신러닝 기법으로 비선형 인과성 분석\n비정상 시계열 및 혼합 주파수 데이터 처리 :\n\n현실에서는 시계열 데이터가 동일하게 수집되지 않는 문제 해결기법 개발\n다중 시간 척도에서 데이터를 통합하여 Granger 인과성 분석\n\n\n- 네트워크 기반 Granger Causality\n\n다변량 시계열에서 여러 변수 간의 상호작용을 고려하여 인과관계 분석\n\n이변량이 아닌 전체 변수(외생변수들의 영향 고려) 네트워크를 분석\nVAR 모델 활용 * \\(x_t=\\sum_{k=1}^{d}A^kX_{t-k}+\\epsilon_t\\) * \\(A_k\\)의 특정요소가 0이 아니면 해당 변수간의 인과성이 존재한다고 판단\n\n\n- 고차원 VAR 모델에서 변수 선택\n\n외생변수 포함하는 경우\n\nFAVOR\nm차원의 요인 \\(f_t\\)를 포함\n직접 관측 X, 최대가능도법 MLE, PCA로 추정\n\n내생변수가 매우 많은 경우\n\n축소 추정으로 VAR 학습\n베이지안 방법론 -&gt; 큰 계수 줄임\nLasso 와 같은 희소성 기법 활용\n\n특정 계수 \\(A_k\\)를 0으로 강제하여 중요하지 않은 변수 제거\n튜닝 파라미터 \\(\\lambda \\geq 0\\) 이 희소성 수준 조절\n값이 클수록 많은 계수 0으로\n$A^k_{ji} $인 k가 있어야함\n\nVAR 모델 손실함수를 정규화하여 해결하는 방식\n\n\n- 이후의 시도들\n\nBasu et al.\nDavis et al"
  },
  {
    "objectID": "posts/1.2024기상데이터전처리blog.html",
    "href": "posts/1.2024기상데이터전처리blog.html",
    "title": "1. 2024 기상데이터 전처리",
    "section": "",
    "text": "import pandas as pd\n\n- 데이터 불러오기\n\nwt= pd.read_csv(\"OBS_ASOS_TIM_20250322224121.csv\",encoding=\"cp949\")\n\n\nwt\n\n\n\n\n\n\n\n\n지점\n지점명\n일시\n기온(°C)\n강수량(mm)\n풍속(m/s)\n습도(%)\n일사(MJ/m2)\n\n\n\n\n0\n146\n전주\n2024-01-01 01:00\n3.8\nNaN\n1.5\n80\nNaN\n\n\n1\n146\n전주\n2024-01-01 02:00\n3.9\nNaN\n0.2\n79\nNaN\n\n\n2\n146\n전주\n2024-01-01 03:00\n3.5\nNaN\n0.4\n84\nNaN\n\n\n3\n146\n전주\n2024-01-01 04:00\n1.9\nNaN\n1.1\n92\nNaN\n\n\n4\n146\n전주\n2024-01-01 05:00\n1.4\nNaN\n1.5\n94\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n146\n전주\n2024-12-30 20:00\n7.6\nNaN\n1.4\n71\nNaN\n\n\n8756\n146\n전주\n2024-12-30 21:00\n7.5\nNaN\n1.7\n69\nNaN\n\n\n8757\n146\n전주\n2024-12-30 22:00\n7.2\nNaN\n1.2\n70\nNaN\n\n\n8758\n146\n전주\n2024-12-30 23:00\n7.2\nNaN\n1.7\n71\nNaN\n\n\n8759\n146\n전주\n2024-12-31 00:00\n7.4\nNaN\n2.8\n70\nNaN\n\n\n\n\n8760 rows × 8 columns\n\n\n\n- null값 확인\n\nwt.isnull().sum()\n\n지점              0\n지점명             0\n일시              0\n기온(°C)          0\n강수량(mm)      7822\n풍속(m/s)         0\n습도(%)           0\n일사(MJ/m2)    3967\ndtype: int64\n\n\n\nwt.columns\n\nIndex(['지점', '지점명', '일시', '기온(°C)', '강수량(mm)', '풍속(m/s)', '습도(%)',\n       '일사(MJ/m2)'],\n      dtype='object')\n\n\n- 필요없는 변수 제거\n\nwt=wt.drop(columns=['지점명','지점'])\n\n\nwt.columns\n\nIndex(['일시', '기온(°C)', '강수량(mm)', '풍속(m/s)', '습도(%)', '일사(MJ/m2)'], dtype='object')\n\n\n- column명 쉽게 변경\n\nwt.columns=['일시','기온','강수량','풍속','습도','일사']\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\nNaN\n1.5\n80\nNaN\n\n\n1\n2024-01-01 02:00\n3.9\nNaN\n0.2\n79\nNaN\n\n\n2\n2024-01-01 03:00\n3.5\nNaN\n0.4\n84\nNaN\n\n\n3\n2024-01-01 04:00\n1.9\nNaN\n1.1\n92\nNaN\n\n\n4\n2024-01-01 05:00\n1.4\nNaN\n1.5\n94\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\nNaN\n1.4\n71\nNaN\n\n\n8756\n2024-12-30 21:00\n7.5\nNaN\n1.7\n69\nNaN\n\n\n8757\n2024-12-30 22:00\n7.2\nNaN\n1.2\n70\nNaN\n\n\n8758\n2024-12-30 23:00\n7.2\nNaN\n1.7\n71\nNaN\n\n\n8759\n2024-12-31 00:00\n7.4\nNaN\n2.8\n70\nNaN\n\n\n\n\n8760 rows × 6 columns\n\n\n\n- null값 제거\n- 강수량, 일사 변수만 null값이 있기때문에 비가 안올 때, 밤에 0으로 측정이 안되었다고 판단\n- 0으로 채움\n\nwt[['강수량', '일사']] = wt[['강수량', '일사']].fillna(0)\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 그래프 크기 설정\nfig, axes = plt.subplots(3, 2, figsize=(12, 12))\naxes = axes.flatten()\n\n# 각 변수별 시각화\nfor i, col in enumerate(wt.columns[1:]):\n    sns.lineplot(data=wt[col], marker='o', ax=axes[i])\n    axes[i].set_title(col)\n    axes[i].set_ylabel(col)\n\n# 빈 그래프 삭제\nfig.delaxes(axes[-1])  \n\nplt.tight_layout()\nplt.show()\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n- csv 생성\n\nwt.to_csv('weather2024.csv', index=False, encoding='utf-8-sig')"
  },
  {
    "objectID": "posts/4.NeuralGrangerCausality.html",
    "href": "posts/4.NeuralGrangerCausality.html",
    "title": "4. Neural Granger Causality",
    "section": "",
    "text": "링크 : Neural Granger Causality\n\n1. 요약\n- 목표\n\n비선형 시계열 데이터에서 Granger Causality를 추론하기 위해, 신경망 기반 모델(MLP,LSTM)을 사용하고, sparsity penalty(희소성 유도 정규화)를 도입한 새로운 방법을 제안\n\n- 주요 내용\n\nGranger causality는 어떤 변수의 과거가 다른 변수의 미래를 예측하는데 도움이 되는지 보는 것\n기존의 대부분의 방법은 선형 모델(VAR) 사용 \\(\\to\\) 비선형관계 잡지 못함\n시간 지연을 명시적으로 지정해야함 \\(\\to\\) 너무 작으면 긴 시간지연에서의 Granger인과관계 누락, 너무 크면 과적합 발생 가능\nComponent-wise neural nets(각 출력 변수마다 독립적인 MLP 또는 LSTM) + structured sparsity penalty를 통해 해석 가능한 인과 구조를 학습\n특히 Group Lasso, Hierarchical Group Lasso 등을 통해 특정 변수의 영향력 제거 \\(\\to\\) 과적합 방지, 시간 지연 자동으로 선택\nDREAM3 유전자 네트워크, 인간 동작 데이터 등에 적용하여 뛰어난 성능을 보임\n\n\n\n2. Linear Granger causality\n- Granger causality : 한 시계열의 과거 값이 다른 시계열이 미래 값을 예측하는데 도움이 되는지를 평가하는 개념\n- VAR(Vector Autogressive Regression)모델\n\nGranger causality를 분석하는 가장 기본적인 방법\n시계열 데이터 표현 \\(x_t=\\sum_{k=1}^{K}A^{(k)}x_{t-k}+\\epsilon_t\\)\n\\(x_{t}\\) : 시점 t에서의 다변량 시계열 데이터(p개의 변수 포함) (p x 1)\nk : 최대 시차\n\\(x_{t-k}\\) : 변수들의 과거값 행렬 (p x k)\n\\(A^{(k)}\\) : 시차 k에서의 각 변수간의 Granger 계수행렬(p x p크기)\n\\(e_t\\) : 평균이 0인 잡음\n\n- 인과성 판단 기준 - 특정 시계열 j가 다른 시계열 i의 미래를 예측하는데 기여하는지를 확인\n\n\\(A^k_{ij}=0\\)이면, 시계열 j는 시계열 i에 대해 Granger 비인과적\n\\(A^k_{ij}\\neq 0\\)이면, 시계열 j는 시계열 i에 대해 Granger 인과적\n\n- 장점\n\n간단하고 해석이 쉬움\n수학적으로 정립되어있고 검정도 가능\n\n- 단점(\\(\\to\\) Neural Granger 등장배경)\n\n선형성 가정\n\n현실 데이터는 대부분 비선형, 선형 구조만 모델링 가능\n\n고정된 lag 수 K 지정\n\n어떤 시계열은 짧은 lag, 어떤 시계열은 긴 lag에 반응할 수 있는데\n모두 동일한 K를 적용함\n\n고차원 문제에 취약\n\n변수가 많아지만 VAR파라미터 수도 폭발적 증가\n과적합 위험\n\n\n- 희소성 유도 패널티\n\n모델의가중치 중 일부를 정확히 0으로 만드는 정규화 기법\n모델이 불필요한 입력을 자동으로 제거하도록 유도하는 제약\n모든 변수들이 서로 영향을 주지 않음, 대부분의 관계는 0인 것이 보통\n\n- 희소성 유도 패널티 적용\n\n너무 많은 변수를 고려하면 과적합문제 발생 \\(\\to\\) Lasso,Group Lasso 패널티를 적용하여 불필요한 계수 0으로\n\nGroup Lasso 패널티 : 결과적으로 희소한 Granger 인과 네트워크를 학습\n\n\\(min_{A(1),...,A(K)}\\sum_{t=K}^{T}||x_t-\\sum_{k=1}^{K}A^{(k)}x_{t-k}||^2_2+\\lambda\\sum_{ij}||(A^{(1)}_{ij},...,A^{K}_{ij}||_2,\\)\n첫 항 : 예측오차 최소화\n둘째 항 : 정규화 항 = 희소성 유도 패널티\n그룹라쏘는 각 시계열 쌍(i,j)에 대해 모든 시간지연 k에 해당하는 계수들을 하나의 그룹으로 보고 동시에 0으로 수축시킴\n입력변수 j의 모든 lag가 출력변수 i에 영향을 주지 않으면 모두 0이 됨 \\(\\to\\) Granger 인과성 없음\n\\(\\lambda\\) : 패널티 강도(크면 더 많은 항이 0)\n\nHierarhical Group Lasso 패널티 : 단순히 인과관계 유무만 판단하는 것이 아니라 인과관계 lag도 자동으로 선택해줌\n\n\n\n\n3. Models for Neural Granger Causality\n\n3.1 Adapting Neural Networks for Granger Causality\n- 비선형 자기 회귀 모델(Nonlinear Autogressive Model, NAR)\n\nNAR : 기존의 선형 VAR보다 훨씬 유연하게 과거-현재 관계 표현가능\n\\(x_t=g(x_{&lt;t1},...,x_{&lt;tp})+\\epsilon_t\\)\n\\(x_t\\) : 시점 t에서의 다변량 시계열 데이터\n\\(x_{&lt;t1},...,x_{&lt;tp}\\) : \\(x_{tj}\\) 시계열 j의 과거값들\n\\(g(*)\\) : 비선형 함수(ex: 신경망)\n\\(e_t\\) : 평균이 0인 잡음(noise)\n\n- Problem\n\n블랙박스 문제 : MLP/LSTM은 강력한 예측성능을 가지지만 전체 시계열을 한번에 모델링(출력 \\(x_t\\)를 하나의 신경망으로 학습) \\(\\to\\) 특정 입력 \\(x_j\\)가 특정 출력 \\(x_i\\)에 어떤 영향을 주는지 분리 어려움\n모든 출력이 같은 시간지연(lag) 에 의존한다고 가정 : 전통적인 VAR모델에서는 최대 시차 k를 설정, 일반적 NAR 딥러닝 모델에서는 입력으로 같은 시간지연만 넣음\n\n모든 시계열이 지난 3시점만 (t-1, t-2, t-3)\n하지만 어떤 변수는 짧은 lag, 어떤 변수는 긴 lag에 의존할 수 있음\n\\(\\to\\) 현실적인 구조 반영 못함\n\n\n- 해결방법\n\ncMLP,cLSTM : Component-wise Neural Network\n각 출력 변수 \\(x_{ti}\\)마다 독립적인 신경망 모델 \\(g_i\\)를 사용하여 입력변수와의 관계를 학습\n\n출력을 하나하나 분리해서 학습\n이러한 구조를 component-wise architecture(그래서 모델 이름 cMLP,cLSTM)\n\n장점 : 어떤 입력 시계열이 출력에 영향을 주는지 쉽게 해석 가능, 각 \\(g_i\\)함수에만 집중하면 되기 때문에 인과 추론이 쉬워짐\n\n- Granger Non-Causality\n\n시계열 j가 시계열 i의 Granger 원인이 아니다 \\(\\leftrightarrow\\) \\(g_i\\) 함수가 \\(x_j\\)의 과거에 전혀 의존하지 않음\n\\(g_i(x_{t1},...,x_{tj},...x_{tp}) = g_i(x_{&lt;ti},...,x^{'}_{&lt;tj},...,x_{tp})\\)\n여기서 \\(x_{&lt;tj} \\neq x^{'}_{&lt;tj}\\)임에도 결과는 같음\n즉, \\(x_j\\)의 과거가 바뀌어도 \\(x_{ti}\\) 예측에 아무 영향이 없음\n\n\n\n3.2 Sparse Input MSPs(cMLP)\n- 구조\n\n각 출력 시계열 \\(x_i\\) 에 대해 별도의 MLP \\(g_i\\) 학습\n희소성(sparsity) 을 유도해서 관련 없는 입력은 가중치를 0으로\n각 출력변수 \\(x_{ti}\\)는 다음과 같이 예측\n\n\\(x_{ti}\\) = \\(g_i(x_{&lt;t})+\\epsilon_{ti}\\)\n\\(g_i(*)\\) : MLP 함수\n입력 : 시간 지연 K 만큼 고려된 전체 시계열의 과거 \\(x_{t-1}, x_{t-2},...x_{t-k}\\)\n출력 : 하나의 시계열 값 \\(x_ti\\)\n\nL개의 층(은닉층 L-1개, 출력층 1개)\n첫 번째 은닉층의 가중치가 가장 중요함 \\(\\to\\) 여기에 인과 구조가 반영\n첫 층의 가중치 \\(W_1\\)은 시간 지연별로 나눔\n\n\\(W^1={W^{1,1}, W^{1,2}, ..., W^{1,K}}\\)\n지연 k별 가중치 \\(W^{1,k}\\) \\(\\to\\) lag별 효과 분석 가능\n\n\n- Granger causality 판별 기준\n\n입력 \\(x_j\\)의 과거값들이 출력 \\(x_i\\)에 영향을 주지 않는다면, 첫 층의 해당 입력에 연결된 모든 가중치가 0이어야함\n\n\\(W^1_{:,j}=0\\) for all lags \\(\\to\\) 시계열 j는 i의 Granger 원인이 아님\n그래서 가중치 0으로 만드는 penalty 도입!\n\nGroup Lasso : 입력 시계열 j의 모든 지연에 대한 가중치를 하나의 그룹으로 보고, 그룹 전체를 0으로 만듦 \\(\\to\\) 시계열 j 전체를 삭제할지 말지 결정\nSparse Group Lasso\n\n더 유연하고 미세한 제어 가능\n일부 시계열은 완전 제거, 일부는 특정 lag만 유지\n\\(\\alpha\\) : 전체 sparsity vs lag 선택간의 조절 파라미터\n\nHierarchical Group Lasso\n\n특정 lag 이상은 모두 제거되도록 하는 penalty\nlag 선택이 자연스럽게 이루어짐, 가중치 0인 관련없는 lag 자동으로 제거\n만약 lag 3의 가중치를 없애려면 lag 4,5…도 자동으로 없어져야함\n\\(\\to\\) lag를 크게 설정해도 됨\n\n\n\n\n3.3 Sparse Input RNNs\n- cLSTM\n\n입력 : 전체 시계열 \\(x_t ∈R^p\\)\n출력 : 오직 한 변수 \\(x_{ti}\\) 만 예측하는 신경망 \\(g_i\\)\n\\(f_t = \\sigma(W_fx_t+U_fh_{t-1})\\)(forget gate)\n\\(i_t = \\sigma(W_inx_t+U_{in}h_{t-1}\\)(input gate)\n\\(o_t = \\sigma(W_ox_t+U_oh_{t-1})\\)(output gate)\n\\(C_t=f_t \\odot C_{t-1}+i_t \\odot tanh(W_cx_t + U_ch_{t-1})\\) (candidate cell state)\n\\(h_t=o_t\\odot tanh(C_t)\\)\n\\(C_t\\) : 셀 상태\n\\(x_t\\) : 현재 입력(전체 시계열)\n\\(h_t\\) : 현재 hidden state\n\\(W_f, W_{in}, W_o,W_c\\) : 입력 가중치 행렬들 각각 shape은 h(hidden unnit 수) X p(변수 수)\n인과 관계는 이 입력 가중치들(W : shape 4h x p)을 통해서 표현됨!!\n\n- Granger causality 추론 방법\n\n시계열 j가 출력 \\(x_{ti}\\)에 영향을 주지 않으려면 입력 가중치 행렬 W의 j번째 column이 모두 0이 되면 됨 \\(\\to\\) Granger 인과하지 않다\n\n\\(W_{:,j}=0\\) \\(\\to\\) 시계열 j는 i의 Granger 원인이 아님\n\nGroup lasso 주로 사용\n\n- cMLP 와 cLSTM 차이\n\ncMLP\n\n고정된 lag(지정 필요)\n계산구조 - 완전 연결 구조 고정된 길이의 과거 시점을 한번에 입력\n첫 층 가중치로 인과 해석\n\ncLSTM\n\n장기 기억 자동 처리 (lag지정 필요없음)\n계산구조 - 순환 구조 입력이 시간적으로 순차적으로 들어감\n입력 가중치 W로 인과 해석\n\n\n\n\n\n4. Optimizing the Penalized Objectives\n\n4.1 Optimizing the Penalized cMLP Objective\n- 최적화 방법 : Proximal Gradient Descent (proximal : 중심의)\n\n희소성 penalty 미분 불가능 or , 정확이 0이되는 가중치 만들기 어려움 \\(\\to\\) sparsity를 정확히 유도하기 어려움\nProximal Gradient는 가중치가 딱 0이 되도록 업데이트\n알고리즘 구조\n\n현재 가중치 \\(W^m\\)에 대해 손실함수 \\(L(W)\\)의 gradient 계산\ngradient만큼 이동(평범한 업데이트)\nproximal 연산(prox)으로 sparsity penalty를 반영한 업데이트(정규화 적용)\n\nprox 연산은 각 penalty종류에 따라 다르게 정의\n\nGroup Lasso : 단순 L1 정규화 column vector 전체를 soft-Thresholding\nSparse Group Lasso : 먼저 개별 lag별 soft-threshold \\(\\to\\) 전체 vector에 soft-threshold\nHierarchical Group Lasso : lag가 높은 그룹부터 차례대로 soft-threshold적용\n\n\n- Soft-thresholding\n\n어떤 값 x에 대해 정규화 항을 적용한 후,\n절댓값이 작으면 0이 됨(가중치 제거)\n크면 그 크기만큼 줄어듦(패널티에 의한 수축)\n\n\\[\nS_{\\lambda}(x) =\n\\begin{cases}\nx - \\lambda & \\text{if } x &gt; \\lambda \\\\\\\\\n0 & \\text{if } |x| \\leq \\lambda \\\\\\\\\nx + \\lambda & \\text{if } x &lt; -\\lambda\n\\end{cases}\n\\]\n\n\n4.2 Optimizing the Penalized cLSTM Objective\n- 최적화 방법 : 마찬가지로 미분 불가능하기 때문에 Proximal Gradient Descent\n- 학습 방법\n\nBackpropagation Through Time(BPTT)로 gradient 계산(모든시점에 대해 그래디언트 계산)\n\nLSTM은 시간 축으로 펼쳐야 하니까 이걸 사용\n시계열 길이가 길경우에 truncated BPTT사용 \\(\\to\\) 길게 펼치지 않고 적당히 잘라서 계산\n\n\n\n\n\n5. Comparing cMLP and cLSTM Models for Granger Causality\n- lag(지연 시간) 처리 방식\n\ncMLP\n\n최대 lag K를 명시적으로 지정\n하지만 hierarchical penalty 덕분에 불필요한 lag는 자동 제거\n\ncLSTM\n\nlag지정할 필요 없음\nLSTM 내부 구조가 장기 의존성 자동 처리\n\n\n- 사용 가능한 데이터량\n\ncMLP\n\n최대 lag K 때문에 훈련에 사용할 수 있는 데이터 수가 T-K\n\ncLSTM\n\ncLSTM은 한 시점 전부터 사용 가능하므로 T-1개 데이터 사용 가능\n\\(\\to\\) 작은 데이터셋에서는 cLSTM이 상대적으로 더 많은 학습데이터 활용 가능\n\n\n- sparsity penalty\n\n둘 다 입력 가중치에 정규화를 걸어 Granger 인과성 없음 \\(\\to\\) 가중치 0 유도\n\n- 한계\n\ncMLP\n\nlag지정 잘못되면 성능 저하\n\ncLSTM\n\n해석력은 좋으나 내부 구조 해석은 다소 복잡\n\n\n\n\n6. Simulation Experiments\n\n6.1 cMLP and cLSTM Simulation Comparison\n- 목적\n\n두 모델이 선형 및 비선형 시계열에서 Granger 인과 그래프를 얼마나 잘 복원하는지\n사용한 시뮬레이션 데이터\n\nLorenz-95 모델 : 비선형, 복잡한 시스템\nVAR(Vector AutoRegressive)모델 : 선형구조\n\n\n\n6.1.1 Lorenz-96 모델 실험\n\n비선형적 다변량 동적 시스템\n\\(\\frac{dx_{ti}}{dt}=(x_{t(i+1)}-x_{t(i-2)})x_{t(i-1)}-x_{ti}+F\\)\nF : 혼란스러움 조절 파라미터 \\(F∈\\{10,40\\}\\)\n\nF=10 : 덜 복잡\nF=40 : 더 chaotic\n\n차원(변수 개수) \\(p=20\\), 시계열 길이 \\(T∈\\{250,500,1000\\}\\)\ncMLP 모델은 시차 lag K=5, hierarchical penalty 사용\n각 설정마다 5번 반복하고 평균을 냈음\n평가 방식: (AUROC)\n\n실제 인과구조 GC 와 예측해낸 인과구조 GC_est가 얼마나 유사한가\n1: Granger 인과성 존재, 0: Granger 인과성 없음\n변수 \\(p\\)개면 \\(p^2\\) 만큼의 인과쌍(자기 자신 제외하면 \\(p^2-p\\))\n\\(p^2-p\\) 에 대한 AUROC\n\nAUROC : treshold를 다양하게 바꿔가며\nx축 1-특이도 : False positive rate\ny축 민감도 : True positive rate\n로 그린 ROC curve의 아래면적\n\n\n\n\n\n\nModel\nT=250\nT=500\nT=1000\n\n\n\n\ncMLP (F=10)\n86.6\n96.6\n98.4\n\n\ncLSTM (F=10)\n81.3\n93.4\n96.0\n\n\ncMLP (F=40)\n84.0\n89.6\n95.5\n\n\ncLSTM (F=40)\n75.1\n87.8\n94.4\n\n\n\n\n전반적으로 cMLP가 높은 정확도 (AUROC)-Grangr 인과구조 복원 정확도 를 보임\n데이터가 많아질수록 성능이 비슷해짐\n\n\n\n6.1.2 VAR 모델 실험\n\nGranger 인과 구조가 정확히 정의된 데이터셋 생성\n모델 생성시 실제 lag가 lag 1 , lag 2인 두가지 데이터 생성 VAR(1), VAR(2)\n선형 시계열 모델, 각 시계열이 다른 시계열들의 과거값에 선형적으로 영향을 받음\n\\(x_t = A_1x_{t-1}+A_2x_{t-2}+...+A_kx_{t-k}+\\epsilon_t\\)\n\\(A_k\\) : 계수 행렬\n차원 \\(p=10\\), 시계열 길이 \\(T∈\\{250,500,1000\\}\\)\nlag K=3\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nT=250 (VAR1)\nT=500 (VAR1)\nT=1000 (VAR1)\nT=250 (VAR2)\nT=500 (VAR2)\nT=1000 (VAR2)\n\n\n\n\ncMLP\n91.6\n94.9\n98.4\n84.4\n88.3\n95.1\n\n\ncLSTM\n88.5\n93.4\n97.6\n83.5\n92.5\n97.8\n\n\n\n\n둘다 매우 높은 정확도, T가 클수록 성능 향상\n적은 데이터일수록 cMLP가 안정적으로 좋음\n데이터가 많으면 cLSTM이 좋을 때도 있음\n\n- 해석\n\nVAR은 선형 구조이므로 cMLP가 더 적합한 구조\nLSTM은 비선형성 + 복잡한 Long-term 구조 학습에 강하지만 단순 선형 시계열에서는 오버피팅이 일어날 수도 있음\n결론 : Granger인과 구조가 명확하고 선형적인 경우 cMLP가 더 정확하고 빠르게 복원가능\n\n\n\n\n6.2 Quantitative Analysis of the Hierarchical penalty\n- 목적\n\nHierarchical Group Lasso penalty가 lag 선택과 Granger 인과관계 복원에 어떤영향을 주는지 정량적 평가하기 위한 실험\nLorenz-95 시뮬레이션 데이터 사용\n입력 lag수 K를 다르게 설정하여 세가지 패널티 방식 비교\n모델 : cMLP\n\n\n\n\nLag K\n5\n10\n10\n\n\n\n\nGROUP\n88.1\n82.5\n80.5\n\n\nMIXED\n90.1\n85.4\n83.3\n\n\nHIER\n95.5\n95.4\n95.2\n\n\n\n- 결과해석\n\nGROUP 과 MIXED는 lag가 커질수록 성능 떨어짐\n\noverfitting 또는 noise\n\nHIER은 거의 떨어지지 않음\n\n\\(\\to\\) lag가 커도 불필요한 lag를 자동으로 제거\n최적 lag 수를 지정할 필요 없어짐\n\n\n\n\n\n7. DREAM challenge\n- 목표\n\ncMLP + 정규화 를 통해 유전자 발현 시계열 데이터로부터 유전자 간 조절 네트워크를 복원할 수 있는지 평가\n\n- 데이터셋 : DREAM3 simulation\n\n비선형 동역학 + 은닉 변수 포함 \\(\\to\\) Granger추정 매우 어려움\n5개의 시뮬레이션 데이터셋 \\(\\to\\) 각각 다른 실제 Granger 인과 그래프 존재\n2개는 대장균 데이터..3개는 효모 데이터..\n각 세트는 p=100개의 유전자 + 21시점에서 각시계열 46번 독립적인 반복추정 : 총시점 - 966\n유전자 사이의 실제 조절네트워크(정답)제공\n\n- 실험 설정\n\n사용 모델 : cMLP\nlag : K = 5\n희소성 정규화 : Hierarchical Group Lasso\n평가 : AUROC\n\n- 결과\n\n매우 복잡하고 비선형적인 데이터임에도 불구하고 중요한 인과 연결들을 거의 정확히 복원\nlag가 중요한 경우에도 hierarchical penalty가 자동으로 lag 선택\n\n\n\n8. Dependencies in human motion capture\n- 목적\n\n실제 인간의 움직임 데이터를 이용해 Neural Granger Causality 모델들이 신체 부위 간의 비선형적, 장기적 인과관계를 복원해보는 실험 \\(\\to\\) 현실 데이터에서 잘 작동하는지 확인\n\n- 분석하고자 한 것\n\n우리가 알고 있는 신체의 구조적 연결(관절 간 영향)과\n모델이 학습한 Granger 인과 구조가 유사한지 비교\n\n- 데이터 : MoCap(Motion Capture)\n\n한 사람의 걷는 동작을 포착한 데이터\n24개의 고유한 신체 부위\n여러 자유도로 측정하여 p=54개의 관절의 각도 및 신체 위치 시계열로 구성됨\nT = 2024개의 시점\n\n- 실험 설정\n\n모델 : cLSTM\n은닉 유닛 수 : H=8\n입력분할 : 전체 시계열 길이를 20짜리 조각으로 나누어 학습\n\n- 결과 분석\n\n\\(\\lambda\\)에 값에 따라 그래프를 그림\nsaprsity가 강해질수록 연결이 줄어들고 핵심 인과구조만 남음\n\\(\\lambda\\) 40까지 키웠을 때 남은 결과\n\n양쪽 무릎 간 상호작용(오른쪽 무릎 \\(\\to\\) 왼쪽 무릎, 왼쪽 무릎 \\(\\to\\) 오른쪽 무릎)\n팔 \\(\\to\\) 팔꿈치 \\(\\to\\) 손\n손 \\(\\leftrightarrow\\) 발 (ex: 발 끝이 닿는 동작 등에서 둘 사이의 인과)\n몸통 중심 \\(\\to\\) 사지 (몸의 움직임이 전체 팔다리로 전파)\n\n시계열 기반 비선형 구조 학습이 실제 사람 움직임 분석에도 효과적이다\n\n\n\n\n9. Conclusion\n- 전체 프레임 워크 요약\n\n출력변수마다 독립적인 신경망 구성 \\(\\to\\) 인과관계 해석 쉬워짐\n입력 가중치에 대한 희소성 정규화 적용 \\(\\to\\) Granger 인과구조 추론 가능\n\n- 성능 요약\n\nDREAM3 챌린지\n\n기존 Granger 추론 기법보다 높은 정확도\n\n사람 움직임 데이터\n\n직관적으로 해석 가능한 신체 부위 간 의존성 구조를 복원\n\n\n- 향후 연구 방향\n\n구조화된 정규화 탐색\n\nhierarchical group lasso 를 최초로 신경망에 도입\n트리구조, 네트워크 구조의 penalty 적용해볼 수도\n\n더 강력한 모델로 확장가능\n\nMLP,LSTM 처럼 기초구조만 사용했지만 다른 딥러닝 모델로 확장가능"
  },
  {
    "objectID": "posts/2.시각화및VAR적합.html",
    "href": "posts/2.시각화및VAR적합.html",
    "title": "2. 시각화 및 VAR 적합",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\nwt= pd.read_csv(\"weather2024.csv\")\n\n\nwt\n\n\n\n\n\n\n\n\n일시\n기온\n강수량\n풍속\n습도\n일사\n\n\n\n\n0\n2024-01-01 01:00\n3.8\n0.0\n1.5\n80\n0.0\n\n\n1\n2024-01-01 02:00\n3.9\n0.0\n0.2\n79\n0.0\n\n\n2\n2024-01-01 03:00\n3.5\n0.0\n0.4\n84\n0.0\n\n\n3\n2024-01-01 04:00\n1.9\n0.0\n1.1\n92\n0.0\n\n\n4\n2024-01-01 05:00\n1.4\n0.0\n1.5\n94\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n2024-12-30 20:00\n7.6\n0.0\n1.4\n71\n0.0\n\n\n8756\n2024-12-30 21:00\n7.5\n0.0\n1.7\n69\n0.0\n\n\n8757\n2024-12-30 22:00\n7.2\n0.0\n1.2\n70\n0.0\n\n\n8758\n2024-12-30 23:00\n7.2\n0.0\n1.7\n71\n0.0\n\n\n8759\n2024-12-31 00:00\n7.4\n0.0\n2.8\n70\n0.0\n\n\n\n\n8760 rows × 6 columns\n\n\n\n\nwt.columns = ['date', 'tem', 'rain', 'wind', 'hum', 'sol']\n\n- 전체 기간 시도표\n\nfor i in range(5):\n    wt.iloc[:,[i+1]].plot(figsize=(15, 3));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 첫 1주일 시도표\n\nfor i in range(5):\n    wt.iloc[:24*7,[i+1]].plot(figsize=(15, 3));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n#ts = pd.Series(dff.iloc[:24*7,[0+1]].values, index=pd.date_range(start='2024-01-01', periods=24, freq='H'))\n\n# 분해 수행 (Additive 모델, 주기성 주기=24시간)\nresult = seasonal_decompose(wt.iloc[:24*14,[0+1]].values, model='additive', period=24)\n\n# 시각화\nresult.plot()\n#plt.suptitle(\"시계열 분해 결과 (추세 + 계절 + 잡음)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nts=wt.iloc[:24*14,[0+1]].values\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\nplot_acf(ts, lags=50, ax=axes[0])\naxes[0].set_title(\"ACF\")\n\nplot_pacf(ts, lags=50, ax=axes[1])\naxes[1].set_title(\"PACF\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nwt.iloc[:,1:].corr()\n\n\n\n\n\n\n\n\ntem\nrain\nwind\nhum\nsol\n\n\n\n\ntem\n1.000000\n0.051619\n0.016475\n-0.056472\n0.348940\n\n\nrain\n0.051619\n1.000000\n0.045720\n0.184801\n-0.070628\n\n\nwind\n0.016475\n0.045720\n1.000000\n-0.311978\n0.284515\n\n\nhum\n-0.056472\n0.184801\n-0.311978\n1.000000\n-0.620501\n\n\nsol\n0.348940\n-0.070628\n0.284515\n-0.620501\n1.000000\n\n\n\n\n\n\n\n\nVAR 모형에 적합\n- \\(X\\) : y의 과거값 및 외생변수, 길이는 24*14\n- \\(y\\) : 예측하고자 하는 일사량, 길이는 24\n\n온도와 일사량\n\n\nX = wt.iloc[:24*14,[1,-1]]\ny = wt.iloc[24*14:24*15,1:]\n\n\nfrom statsmodels.tsa.api import VAR\nfrom sklearn.metrics import mean_squared_error\n# 데이터: y, x1~x4가 있는 시계열 데이터프레임\n# 인덱스는 datetime이어야 함\n\n# 1. 정상성 검사 및 필요시 차분 (여기선 생략, 필요시 추가해줘)\n\n# 2. VAR 모형 적합\nmodel = VAR(X)\n\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=48)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\n#print(results.summary())\n\nAIC 기준 최적 시차: 21\n\n\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast1 = results.forecast(y=forecast_input, steps=n_forecast)\n\n\n습도와 일사량\n\n\nX = wt.iloc[:24*14,[-2,-1]]\n#y = wt.iloc[24*7:24*8,1:]\n\nmodel = VAR(X)\n-0\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=24)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\nresults.summary()\n\nAIC 기준 최적 시차: 21\n\n\n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Tue, 01, Apr, 2025\nTime:                     07:26:53\n--------------------------------------------------------------------\nNo. of Equations:         2.00000    BIC:                  0.0867873\nNobs:                     315.000    HQIC:                 -0.528393\nLog likelihood:          -660.240    FPE:                   0.392863\nAIC:                    -0.937725    Det(Omega_mle):        0.304156\n--------------------------------------------------------------------\nResults for equation hum\n==========================================================================\n             coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------\nconst          12.877262         3.048607            4.224           0.000\nL1.hum          1.034009         0.064842           15.947           0.000\nL1.sol         -6.288329         1.841606           -3.415           0.001\nL2.hum         -0.085306         0.091430           -0.933           0.351\nL2.sol          2.476996         2.761977            0.897           0.370\nL3.hum         -0.105987         0.091506           -1.158           0.247\nL3.sol          4.269964         2.810271            1.519           0.129\nL4.hum          0.104119         0.091715            1.135           0.256\nL4.sol         -3.066122         2.814244           -1.090           0.276\nL5.hum          0.020022         0.091322            0.219           0.826\nL5.sol          0.726784         2.790890            0.260           0.795\nL6.hum         -0.154890         0.090401           -1.713           0.087\nL6.sol         -1.001202         2.651443           -0.378           0.706\nL7.hum          0.063325         0.090179            0.702           0.483\nL7.sol         -1.789725         2.574931           -0.695           0.487\nL8.hum          0.013035         0.090204            0.145           0.885\nL8.sol          2.753985         2.559143            1.076           0.282\nL9.hum         -0.079469         0.090351           -0.880           0.379\nL9.sol         -2.534441         2.565794           -0.988           0.323\nL10.hum        -0.021562         0.090809           -0.237           0.812\nL10.sol         0.092242         2.566511            0.036           0.971\nL11.hum         0.101221         0.090510            1.118           0.263\nL11.sol         0.270904         2.559397            0.106           0.916\nL12.hum        -0.044597         0.090874           -0.491           0.624\nL12.sol        -0.931457         2.592624           -0.359           0.719\nL13.hum        -0.049559         0.090879           -0.545           0.586\nL13.sol         0.019081         2.638249            0.007           0.994\nL14.hum         0.071637         0.090692            0.790           0.430\nL14.sol        -1.801443         2.654979           -0.679           0.497\nL15.hum        -0.159492         0.091383           -1.745           0.081\nL15.sol        -1.397162         2.654903           -0.526           0.599\nL16.hum         0.130378         0.091520            1.425           0.154\nL16.sol         3.921220         2.642805            1.484           0.138\nL17.hum         0.125505         0.091874            1.366           0.172\nL17.sol        -0.658568         2.657127           -0.248           0.804\nL18.hum        -0.039232         0.092146           -0.426           0.670\nL18.sol         0.307973         2.647252            0.116           0.907\nL19.hum        -0.033336         0.091821           -0.363           0.717\nL19.sol        -2.210109         2.647698           -0.835           0.404\nL20.hum        -0.091010         0.091946           -0.990           0.322\nL20.sol         1.522105         2.596331            0.586           0.558\nL21.hum         0.060905         0.065948            0.924           0.356\nL21.sol        -3.586148         1.718873           -2.086           0.037\n==========================================================================\n\nResults for equation sol\n==========================================================================\n             coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------\nconst          -0.013084         0.108455           -0.121           0.904\nL1.hum         -0.004107         0.002307           -1.780           0.075\nL1.sol          1.190143         0.065516           18.166           0.000\nL2.hum          0.001161         0.003253            0.357           0.721\nL2.sol         -0.319998         0.098258           -3.257           0.001\nL3.hum          0.003093         0.003255            0.950           0.342\nL3.sol         -0.013066         0.099976           -0.131           0.896\nL4.hum         -0.002233         0.003263           -0.685           0.494\nL4.sol         -0.105982         0.100118           -1.059           0.290\nL5.hum          0.000845         0.003249            0.260           0.795\nL5.sol          0.007535         0.099287            0.076           0.940\nL6.hum          0.004978         0.003216            1.548           0.122\nL6.sol         -0.002949         0.094326           -0.031           0.975\nL7.hum         -0.002640         0.003208           -0.823           0.410\nL7.sol          0.000148         0.091604            0.002           0.999\nL8.hum         -0.003149         0.003209           -0.981           0.326\nL8.sol          0.058175         0.091042            0.639           0.523\nL9.hum          0.002759         0.003214            0.858           0.391\nL9.sol          0.043017         0.091279            0.471           0.637\nL10.hum        -0.000294         0.003231           -0.091           0.927\nL10.sol        -0.032885         0.091305           -0.360           0.719\nL11.hum        -0.000207         0.003220           -0.064           0.949\nL11.sol        -0.080112         0.091052           -0.880           0.379\nL12.hum         0.001996         0.003233            0.617           0.537\nL12.sol        -0.010646         0.092234           -0.115           0.908\nL13.hum         0.002650         0.003233            0.820           0.412\nL13.sol         0.054054         0.093857            0.576           0.565\nL14.hum        -0.007051         0.003226           -2.185           0.029\nL14.sol         0.042851         0.094452            0.454           0.650\nL15.hum         0.004732         0.003251            1.456           0.146\nL15.sol         0.005935         0.094449            0.063           0.950\nL16.hum        -0.003122         0.003256           -0.959           0.338\nL16.sol        -0.081783         0.094019           -0.870           0.384\nL17.hum         0.000985         0.003268            0.301           0.763\nL17.sol         0.018094         0.094528            0.191           0.848\nL18.hum        -0.000796         0.003278           -0.243           0.808\nL18.sol        -0.070061         0.094177           -0.744           0.457\nL19.hum         0.003162         0.003267            0.968           0.333\nL19.sol        -0.020653         0.094193           -0.219           0.826\nL20.hum        -0.002413         0.003271           -0.738           0.461\nL20.sol         0.080876         0.092365            0.876           0.381\nL21.hum         0.000457         0.002346            0.195           0.846\nL21.sol         0.107119         0.061150            1.752           0.080\n==========================================================================\n\nCorrelation matrix of residuals\n            hum       sol\nhum    1.000000 -0.387709\nsol   -0.387709  1.000000\n\n\n\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast2 = results.forecast(y=forecast_input, steps=n_forecast)\n\n\n풍속과 일사량\n\n\nX = wt.iloc[:24*14,[-3,-1]]\n#y = wt.iloc[24*7:24*8,1:]\n\nmodel = VAR(X)\n\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=24)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\nresults.summary()\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast2_ = results.forecast(y=forecast_input, steps=n_forecast)\n\nAIC 기준 최적 시차: 21\n\n\n\n강수량과 일사량\n\n\nX = wt.iloc[:24*14,[-4,-1]]\n#y = wt.iloc[24*7:24*8,1:]\n\nmodel = VAR(X)\n\n# 3. 시차 선택\nlag_result = model.select_order(maxlags=24)\nprint(\"AIC 기준 최적 시차:\", lag_result.selected_orders['aic'])\nlag = lag_result.selected_orders['aic']\n\n# 4. VAR 모델 적합\nresults = model.fit(lag)\nresults.summary()\n\nn_forecast = 24\nforecast_input = X.values[-lag:]  # 최근 lag만큼의 데이터 사용\nforecast3_ = results.forecast(y=forecast_input, steps=n_forecast)\n\nAIC 기준 최적 시차: 20\n\n\n\nAR 모형 일사량만 이용\n\n\nfrom statsmodels.tsa.ar_model import AutoReg\n\n# 예시: y 시계열 데이터프레임 (datetime index 권장)\n# df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=True)\n# y = df['y']\ny_ = wt.iloc[:24*14,[-1]]# 시계열 형태로 가져오기\n\n# 1. 시차(p) 설정 또는 자동 선택\np = 24  # 최근 24시간을 기반으로 다음을 예측한다고 가정\n\n# 2. AR 모형 적합\nmodel = AutoReg(y_, lags=p, old_names=False)\nresults = model.fit()\n\n# 3. 다음 24시간 예측\nforecast3 = results.predict(start=len(y), end=len(y)+23)\n\n\nprint('온도, 일사량',mean_squared_error(y['sol'].values,forecast1[:,1]))\nprint('습도, 일사량',mean_squared_error(y['sol'].values,forecast2[:,1]))\nprint('풍속, 일사량',mean_squared_error(y['sol'].values,forecast2_[:,1]))\nprint('강수량, 일사량',mean_squared_error(y['sol'].values,forecast3_[:,1]))\n\nprint('일사량',mean_squared_error(y['sol'].values,forecast3))\n\n\n\nplt.figure(figsize = (15,7))\nplt.plot(y['sol'].values, label = 'y',color = 'black')\nplt.plot(forecast1[:,1],label = 'tem+sol')\nplt.plot(forecast2[:,1],label = 'hum+sol')\nplt.plot(forecast2_[:,1],label = 'wind+sol')\nplt.plot(forecast3_[:,1],label = 'rain+sol')\n\nplt.plot(forecast3.values,label = 'sol')\nplt.legend()\nplt.show()\n\n온도, 일사량 0.20502706789905156\n습도, 일사량 0.2600785050783011\n풍속, 일사량 0.08792570082191763\n강수량, 일사량 0.3652957905544259\n일사량 0.4390186400449338\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 0. y_ 인덱스가 숫자면 → datetime 인덱스로 바꿔주기\nif not isinstance(y_.index[0], pd.Timestamp):\n    y_.index = pd.date_range(start='2024-01-01', periods=len(y_), freq='h')\n\n# 1. 예측 구간 인덱스 생성\nforecast_index = pd.date_range(start=y_.index[-1] + pd.Timedelta(hours=1), periods=24, freq='h')\n\n# 2. 예측값 시리즈화\nsol = pd.Series(y['sol'].values, index=forecast_index)\ntem_sol = pd.Series(forecast1[:, 1], index=forecast_index)\nhum_sol = pd.Series(forecast2[:, 1], index=forecast_index)\nsol_only = pd.Series(forecast3.values.flatten(), index=forecast_index)\n\n# 3. 플롯\nplt.figure(figsize=(15, 7))\nplt.plot(y_[-24*7:],color = 'black')\nplt.plot(sol, label = 'y',color = 'black')\n#plt.plot(tem_sol, label='tem+sol')\nplt.plot(hum_sol, label='hum+sol')\nplt.plot(sol_only, label='sol only')\nplt.legend()\n#plt.title(\"과거 + 예측 시계열\")\n#plt.xlabel(\"시간\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n- Granger 인과검정\n\n각 변수 상관관계\n풀모형에서 Granger 인과검정\n\n\\(H_0\\) : 외생변수 전체는 Granger 인과하지 않다\n\\(H_1\\) : 외생변수 중 어떤 하나 이상의 변수가 Granger 인과하다\n\n\n\nfrom statsmodels.tsa.api import VAR\n\n# 1. 전체 시계열 데이터 사용 (예: df = ['y', 'x1', 'x2', 'x3', 'x4'])\n\nX = wt.iloc[:24*14,1:]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic')  # 자동 시차 선택\n\ngc_test = results.test_causality(caused='sol', causing=['tem', 'rain', 'wind', 'hum'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: ['tem', 'rain', 'wind', 'hum'] do not Granger-cause sol. Conclusion: reject H_0 at 5% significance level.\n================================================\nTest statistic Critical value p-value     df    \n------------------------------------------------\n         2.683          1.758   0.001 (12, 1585)\n------------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[1,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['tem'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: tem does not Granger-cause sol. Conclusion: reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n         2.323          1.590   0.001 (20, 550)\n-----------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[2,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['rain'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: rain does not Granger-cause sol. Conclusion: reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n         3.284          1.590   0.000 (20, 550)\n-----------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[3,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['wind'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: wind does not Granger-cause sol. Conclusion: fail to reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n        0.8684          1.575   0.633 (21, 544)\n-----------------------------------------------\n\n\n\nX = wt.iloc[:24*14,[4,-1]]\nmodel = VAR(X)\nresults = model.fit(maxlags=24,ic='aic') \ngc_test = results.test_causality(caused='sol', causing=['hum'], kind='f')\n\n# 3. 결과 요약\nprint(gc_test.summary())\n\nGranger causality F-test. H_0: hum does not Granger-cause sol. Conclusion: fail to reject H_0 at 5% significance level.\n===============================================\nTest statistic Critical value p-value     df   \n-----------------------------------------------\n        0.9374          1.575   0.542 (21, 544)\n-----------------------------------------------\n\n\n\nfig, ax = plt.subplots(5,1,figsize=(12,10))\nfor i in range(5):\n    ax[i].plot(wt.iloc[:24*14,i+1])"
  }
]